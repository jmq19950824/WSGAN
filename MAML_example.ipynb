{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T12:15:01.698958Z",
     "start_time": "2020-11-14T12:15:01.003704Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# root_path = './../datasets'\n",
    "# processed_folder =  os.path.join(root_path)\n",
    "\n",
    "# zip_ref = zipfile.ZipFile(os.path.join(root_path,'omniglot.zip'), 'r')\n",
    "# zip_ref.extractall(root_path)\n",
    "# zip_ref.close()\n",
    "\n",
    "root_dir = 'E:/Research_Anomaly Detection/20201111_formal version/Untitled Folder/MAML_example/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T12:15:01.713819Z",
     "start_time": "2020-11-14T12:15:01.703928Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 数据预处理\n",
    "\n",
    "# import torchvision.transforms as transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# '''\n",
    "# an example of img_items:\n",
    "# ( '0709_17.png',\n",
    "#   'Alphabet_of_the_Magi/character01',\n",
    "#   './../datasets/omniglot/python/images_background/Alphabet_of_the_Magi/character01')\n",
    "# '''\n",
    "# def find_classes(root_dir):\n",
    "#     img_items = []\n",
    "#     for (root, dirs, files) in os.walk(root_dir): \n",
    "#         for file in files:\n",
    "#             if (file.endswith(\"png\")):\n",
    "#                 r = root.split('/')\n",
    "#                 img_items.append((file, r[-2] + \"/\" + r[-1], root))\n",
    "#     print(\"== Found %d items \" % len(img_items))\n",
    "#     return img_items\n",
    "\n",
    "# ## 构建一个词典{class:idx}\n",
    "# def index_classes(items):\n",
    "#     class_idx = {}\n",
    "#     count = 0\n",
    "#     for item in items:\n",
    "#         if item[1] not in class_idx:\n",
    "#             class_idx[item[1]] = count\n",
    "#             count += 1\n",
    "#     print('== Found {} classes'.format(len(class_idx)))\n",
    "#     return class_idx\n",
    "        \n",
    "\n",
    "# img_items =  find_classes(root_dir)\n",
    "# class_idx = index_classes(img_items)\n",
    "\n",
    "\n",
    "# temp = dict()\n",
    "# for imgname, classes, dirs in img_items:\n",
    "#     img = '{}/{}'.format(dirs, imgname)\n",
    "#     label = class_idx[classes]\n",
    "#     transform = transforms.Compose([lambda img: Image.open(img).convert('L'),\n",
    "#                               lambda img: img.resize((28,28)),\n",
    "#                               lambda img: np.reshape(img, (28,28,1)),\n",
    "#                               lambda img: np.transpose(img, [2,0,1]),\n",
    "#                               lambda img: img/255.\n",
    "#                               ])\n",
    "#     img = transform(img)\n",
    "#     if label in temp.keys():\n",
    "#         temp[label].append(img)\n",
    "#     else:\n",
    "#         temp[label] = [img]\n",
    "# print('begin to generate omniglot.npy')\n",
    "# ## 移除标签信息，每个标签包含20个样本\n",
    "# img_list = []\n",
    "# for label, imgs in temp.items():\n",
    "#     img_list.append(np.array(imgs))\n",
    "# img_list = np.array(img_list).astype(np.float) # [[20 imgs],..., 1623 classes in total]\n",
    "# print('data shape:{}'.format(img_list.shape)) # (1623, 20, 1, 28, 28)\n",
    "# temp = []\n",
    "# np.save(os.path.join(root_dir, 'omniglot.npy'), img_list)\n",
    "# print('end.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T12:15:01.868584Z",
     "start_time": "2020-11-14T12:15:01.726782Z"
    }
   },
   "outputs": [],
   "source": [
    "img_list = np.load(os.path.join(root_dir, 'omniglot.npy')) # (1623, 20, 1, 28, 28)\n",
    "x_train = img_list[:1200]\n",
    "x_test = img_list[1200:]\n",
    "num_classes = img_list.shape[0]\n",
    "datasets = {'train': x_train, 'test': x_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T12:15:02.211942Z",
     "start_time": "2020-11-14T12:15:01.873409Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB: train (1200, 20, 1, 28, 28) test (423, 20, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "### 准备数据迭代器\n",
    "n_way = 5\n",
    "k_spt = 1  ## support data 的个数\n",
    "k_query = 15 ## query data 的个数\n",
    "imgsz = 28\n",
    "resize = imgsz\n",
    "task_num = 8\n",
    "batch_size = task_num\n",
    "\n",
    "indexes = {\"train\": 0, \"test\": 0}\n",
    "datasets = {\"train\": x_train, \"test\": x_test}\n",
    "print(\"DB: train\", x_train.shape, \"test\", x_test.shape)\n",
    "\n",
    "\n",
    "def load_data_cache(dataset):\n",
    "    \"\"\"\n",
    "    Collects several batches data for N-shot learning\n",
    "    :param dataset: [cls_num, 20, 84, 84, 1]\n",
    "    :return: A list with [support_set_x, support_set_y, target_x, target_y] ready to be fed to our networks\n",
    "    \"\"\"\n",
    "    #  take 5 way 1 shot as example: 5 * 1\n",
    "    setsz = k_spt * n_way\n",
    "    querysz = k_query * n_way\n",
    "    data_cache = []\n",
    "\n",
    "    # print('preload next 10 caches of batch_size of batch.')\n",
    "    for sample in range(10):  # num of epochs\n",
    "\n",
    "        x_spts, y_spts, x_qrys, y_qrys = [], [], [], []\n",
    "        for i in range(batch_size):  # one batch means one set\n",
    "\n",
    "            x_spt, y_spt, x_qry, y_qry = [], [], [], []\n",
    "            selected_cls = np.random.choice(dataset.shape[0], n_way, replace =  False) \n",
    "\n",
    "            for j, cur_class in enumerate(selected_cls):\n",
    "\n",
    "                selected_img = np.random.choice(20, k_spt + k_query, replace = False)\n",
    "\n",
    "                # 构造support集和query集\n",
    "                x_spt.append(dataset[cur_class][selected_img[:k_spt]])\n",
    "                x_qry.append(dataset[cur_class][selected_img[k_spt:]])\n",
    "                y_spt.append([j for _ in range(k_spt)])\n",
    "                y_qry.append([j for _ in range(k_query)])\n",
    "\n",
    "            # shuffle inside a batch\n",
    "            perm = np.random.permutation(n_way * k_spt)\n",
    "            x_spt = np.array(x_spt).reshape(n_way * k_spt, 1, resize, resize)[perm]\n",
    "            y_spt = np.array(y_spt).reshape(n_way * k_spt)[perm]\n",
    "            perm = np.random.permutation(n_way * k_query)\n",
    "            x_qry = np.array(x_qry).reshape(n_way * k_query, 1, resize, resize)[perm]\n",
    "            y_qry = np.array(y_qry).reshape(n_way * k_query)[perm]\n",
    " \n",
    "            # append [sptsz, 1, 84, 84] => [batch_size, setsz, 1, 84, 84]\n",
    "            x_spts.append(x_spt)\n",
    "            y_spts.append(y_spt)\n",
    "            x_qrys.append(x_qry)\n",
    "            y_qrys.append(y_qry)\n",
    "\n",
    "#         print(x_spts[0].shape)\n",
    "        # [b, setsz = n_way * k_spt, 1, 84, 84]\n",
    "        x_spts = np.array(x_spts).astype(np.float32).reshape(batch_size, setsz, 1, resize, resize)\n",
    "        y_spts = np.array(y_spts).astype(np.int).reshape(batch_size, setsz)\n",
    "        # [b, qrysz = n_way * k_query, 1, 84, 84]\n",
    "        x_qrys = np.array(x_qrys).astype(np.float32).reshape(batch_size, querysz, 1, resize, resize)\n",
    "        y_qrys = np.array(y_qrys).astype(np.int).reshape(batch_size, querysz)\n",
    "#         print(x_qrys.shape)\n",
    "        data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n",
    "\n",
    "    return data_cache\n",
    "\n",
    "datasets_cache = {\"train\": load_data_cache(x_train),  # current epoch data cached\n",
    "                       \"test\": load_data_cache(x_test)}\n",
    "\n",
    "def next(mode='train'):\n",
    "    \"\"\"\n",
    "    Gets next batch from the dataset with name.\n",
    "    :param mode: The name of the splitting (one of \"train\", \"val\", \"test\")\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # update cache if indexes is larger than len(data_cache)\n",
    "    if indexes[mode] >= len(datasets_cache[mode]):\n",
    "        indexes[mode] = 0\n",
    "        datasets_cache[mode] = load_data_cache(datasets[mode])\n",
    "\n",
    "    next_batch = datasets_cache[mode][indexes[mode]]\n",
    "    indexes[mode] += 1\n",
    "\n",
    "    return next_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T11:36:33.559943Z",
     "start_time": "2020-11-15T11:36:33.548970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2, 4, 1, 0],\n",
       "       [0, 4, 2, 3, 1],\n",
       "       [0, 4, 1, 2, 3],\n",
       "       [1, 2, 0, 4, 3],\n",
       "       [2, 0, 4, 1, 3],\n",
       "       [2, 4, 3, 0, 1],\n",
       "       [3, 1, 2, 4, 0],\n",
       "       [1, 4, 0, 3, 2]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_cache['train'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T12:15:02.289562Z",
     "start_time": "2020-11-14T12:15:02.214935Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from copy import deepcopy,copy\n",
    "        \n",
    "\n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.vars = nn.ParameterList()  ## 包含了所有需要被优化的tensor\n",
    "        self.vars_bn = nn.ParameterList()\n",
    "        \n",
    "        # 第1个conv2d\n",
    "        # in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 1, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        # 第1个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "        \n",
    "        # 第2个conv2d\n",
    "        # in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        # 第2个BatchNorm层exten\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "        \n",
    "        # 第3个conv2d\n",
    "        # in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        # 第3个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "        \n",
    "        # 第4个conv2d\n",
    "        # in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        # 第4个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "        \n",
    "        ##linear\n",
    "        weight = nn.Parameter(torch.ones([5,64]))\n",
    "        bias = nn.Parameter(torch.zeros(5))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),\n",
    "            \n",
    "#             nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),\n",
    "            \n",
    "#             nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2), \n",
    "            \n",
    "#             nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2), \n",
    "            \n",
    "#             FlattenLayer(),\n",
    "#             nn.Linear(64,5)\n",
    "#         )        \n",
    " \n",
    "        \n",
    "    def forward(self, x, params = None, bn_training=True):\n",
    "        '''\n",
    "        :bn_training: set False to not update\n",
    "        :return: \n",
    "        '''\n",
    "        if params is None:\n",
    "            params = self.vars\n",
    "        \n",
    "        weight, bias = params[0], params[1]  # 第1个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride = 2, padding = 2)\n",
    "        \n",
    "        weight, bias = params[2], params[3]  # 第1个BN层\n",
    "        running_mean, running_var = self.vars_bn[0], self.vars_bn[1]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight,bias =bias, training= bn_training)\n",
    "        x = F.max_pool2d(x,kernel_size=2)  #第1个MAX_POOL层  \n",
    "        x = F.relu(x, inplace = [True])  #第1个relu\n",
    "        \n",
    "        \n",
    "        weight, bias = params[4], params[5]  # 第2个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride = 2, padding = 2)\n",
    "        \n",
    "        weight, bias = params[6], params[7]  # 第2个BN层\n",
    "        running_mean, running_var = self.vars_bn[2], self.vars_bn[3]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight,bias =bias, training= bn_training)\n",
    "        x = F.max_pool2d(x,kernel_size=2)  #第2个MAX_POOL层   \n",
    "        x = F.relu(x, inplace = [True])  #第2个relu\n",
    "        \n",
    "        weight, bias = params[8], params[9]  # 第3个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride = 2, padding = 2)\n",
    "        \n",
    "        weight, bias = params[10], params[11]  # 第3个BN层\n",
    "        running_mean, running_var = self.vars_bn[4], self.vars_bn[5]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight,bias =bias, training= bn_training)\n",
    "        x = F.max_pool2d(x,kernel_size=2)  #第3个MAX_POOL层\n",
    "        x = F.relu(x, inplace = [True])  #第3个relu\n",
    "        \n",
    "        weight, bias = params[12], params[13]  # 第4个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride = 2, padding = 2)\n",
    "        x = F.relu(x, inplace = [True])  #第4个relu\n",
    "        weight, bias = params[14], params[15]  # 第4个BN层\n",
    "        running_mean, running_var = self.vars_bn[6], self.vars_bn[7]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight,bias =bias, training= bn_training)\n",
    "        x = F.max_pool2d(x,kernel_size=2)  #第4个MAX_POOL层\n",
    "        \n",
    "        \n",
    "        x = x.view(x.size(0), -1) ## flatten\n",
    "        weight, bias = params[16], params[17]  # linear\n",
    "        x = F.linear(x, weight, bias)\n",
    "        \n",
    "        output = x\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def parameters(self):\n",
    "        \n",
    "        return self.vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T12:15:02.352050Z",
     "start_time": "2020-11-14T12:15:02.296500Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.update_step = 5 ## task-level inner update steps\n",
    "        self.update_step_test = 5  \n",
    "        self.net = BaseNet()\n",
    "        self.meta_lr = 2e-4\n",
    "        self.base_lr = 4 * 1e-2\n",
    "        self.inner_lr = 0.4\n",
    "        self.outer_lr = 1e-2\n",
    "        self.meta_optim = torch.optim.Adam(self.net.parameters(), lr = self.meta_lr)\n",
    "        \n",
    "    def forward(self,x_spt, y_spt, x_qry, y_qry):\n",
    "        # 初始化\n",
    "        task_num, ways, shots, h, w = x_spt.size()\n",
    "        print(f'task num:{task_num}')\n",
    "        query_size = x_qry.size(1) # 75 = 15 * 5\n",
    "        loss_list_qry = [0 for _ in range(self.update_step + 1)]\n",
    "        correct_list = [0 for _ in range(self.update_step + 1)]\n",
    "        \n",
    "        for i in range(task_num):\n",
    "            ## 第0步更新\n",
    "            y_hat = self.net(x_spt[i], params = None, bn_training=True) # (ways * shots, ways)\n",
    "            loss = F.cross_entropy(y_hat, y_spt[i]) \n",
    "            grad = torch.autograd.grad(loss, self.net.parameters())\n",
    "            tuples = zip(grad, self.net.parameters()) ## 将梯度和参数\\theta一一对应起来\n",
    "            # fast_weights这一步相当于求了一个\\theta - \\alpha*\\nabla(L)\n",
    "            fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))\n",
    "            # 在query集上测试，计算准确率\n",
    "            # 这一步使用更新前的数据\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net(x_qry[i], self.net.parameters(), bn_training = True)\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[0] += loss_qry\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "                correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                correct_list[0] += correct\n",
    "            \n",
    "            # 使用更新后的数据在query集上测试。\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net(x_qry[i], fast_weights, bn_training = True)\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[1] += loss_qry\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "                correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                correct_list[1] += correct   \n",
    "            \n",
    "            for k in range(1, self.update_step):\n",
    "                \n",
    "                y_hat = self.net(x_spt[i], params = fast_weights, bn_training=True)\n",
    "                loss = F.cross_entropy(y_hat, y_spt[i])\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                tuples = zip(grad, fast_weights) \n",
    "                fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))\n",
    "                    \n",
    "                y_hat = self.net(x_qry[i], params = fast_weights, bn_training = True)\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[k+1] += loss_qry\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    pred_qry = F.softmax(y_hat,dim=1).argmax(dim=1)\n",
    "                    correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                    correct_list[k+1] += correct\n",
    "#         print('hello')\n",
    "        print(f'loss_list_qry:{loss_list_qry}')\n",
    "        loss_qry = loss_list_qry[-1] / task_num\n",
    "        print(f'loss_qry:{loss_qry}')\n",
    "        \n",
    "        self.meta_optim.zero_grad() # 梯度清零\n",
    "        print(loss_qry.grad)\n",
    "        \n",
    "        loss_qry.backward()\n",
    "        self.meta_optim.step()\n",
    "        \n",
    "        accs = np.array(correct_list) / (query_size * task_num)\n",
    "        loss = np.array(loss_list_qry) / ( task_num)\n",
    "        return accs,loss\n",
    "\n",
    "    \n",
    "    \n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        assert len(x_spt.shape) == 4\n",
    "        \n",
    "        query_size = x_qry.size(0)\n",
    "        correct_list = [0 for _ in range(self.update_step_test + 1)]\n",
    "        \n",
    "        new_net = deepcopy(self.net)\n",
    "        y_hat = new_net(x_spt)\n",
    "        loss = F.cross_entropy(y_hat, y_spt)\n",
    "        grad = torch.autograd.grad(loss, new_net.parameters())\n",
    "        fast_weights = list(map(lambda p:p[1] - self.base_lr * p[0], zip(grad, new_net.parameters())))\n",
    "        \n",
    "        # 在query集上测试，计算准确率\n",
    "        # 这一步使用更新前的数据\n",
    "        with torch.no_grad():\n",
    "            y_hat = new_net(x_qry,  params = new_net.parameters(), bn_training = True)\n",
    "            pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "            correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "            correct_list[0] += correct\n",
    "\n",
    "        # 使用更新后的数据在query集上测试。\n",
    "        with torch.no_grad():\n",
    "            y_hat = new_net(x_qry, params = fast_weights, bn_training = True)\n",
    "            pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "            correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "            correct_list[1] += correct\n",
    "\n",
    "        for k in range(1, self.update_step_test):\n",
    "            y_hat = new_net(x_spt, params = fast_weights, bn_training=True)\n",
    "            loss = F.cross_entropy(y_hat, y_spt)\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            fast_weights = list(map(lambda p:p[1] - self.base_lr * p[0], zip(grad, fast_weights)))\n",
    "            \n",
    "            y_hat = new_net(x_qry, fast_weights, bn_training=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "                correct_list[k+1] += correct\n",
    "                \n",
    "        del new_net\n",
    "        accs = np.array(correct_list) / query_size\n",
    "        return accs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T12:15:02.367009Z",
     "start_time": "2020-11-14T12:15:02.358859Z"
    }
   },
   "outputs": [],
   "source": [
    "# net = torch.load('./trained_models/MTL-5000epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T12:20:36.318106Z",
     "start_time": "2020-11-14T12:15:02.371003Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/60000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8755), tensor(12.2810), tensor(11.8696, grad_fn=<AddBackward0>), tensor(11.4791, grad_fn=<AddBackward0>), tensor(11.3354, grad_fn=<AddBackward0>), tensor(11.0917, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.386460304260254\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "[0.2        0.38       0.41666667 0.455      0.43833333 0.43166667]\n",
      "[tensor(1.6094) tensor(1.5351) tensor(1.4837, grad_fn=<DivBackward0>)\n",
      " tensor(1.4349, grad_fn=<DivBackward0>)\n",
      " tensor(1.4169, grad_fn=<DivBackward0>)\n",
      " tensor(1.3865, grad_fn=<DivBackward0>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                          | 1/60000 [03:25<3421:00:25, 205.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2003 0.3179 0.3662 0.3782 0.3914 0.4028]\n",
      "task num:8\n",
      "loss_list_qry:[tensor(12.8743), tensor(12.2614), tensor(11.7300, grad_fn=<AddBackward0>), tensor(11.3720, grad_fn=<AddBackward0>), tensor(10.9975, grad_fn=<AddBackward0>), tensor(10.6230, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.3278741836547852\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                          | 2/60000 [03:27<2405:34:50, 144.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8753), tensor(12.4253), tensor(11.9875, grad_fn=<AddBackward0>), tensor(11.6829, grad_fn=<AddBackward0>), tensor(11.5174, grad_fn=<AddBackward0>), tensor(11.3720, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.421498417854309\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                          | 3/60000 [03:29<1693:36:28, 101.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8765), tensor(12.2778), tensor(11.7326, grad_fn=<AddBackward0>), tensor(11.3857, grad_fn=<AddBackward0>), tensor(11.1118, grad_fn=<AddBackward0>), tensor(10.8351, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.3543879985809326\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                           | 4/60000 [03:31<1195:51:20, 71.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8770), tensor(12.3439), tensor(11.8621, grad_fn=<AddBackward0>), tensor(11.4475, grad_fn=<AddBackward0>), tensor(11.1298, grad_fn=<AddBackward0>), tensor(10.8957, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.3619580268859863\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 5/60000 [03:33<847:58:30, 50.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8701), tensor(12.5377), tensor(12.2428, grad_fn=<AddBackward0>), tensor(12.0492, grad_fn=<AddBackward0>), tensor(11.7569, grad_fn=<AddBackward0>), tensor(11.4746, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.4343204498291016\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 6/60000 [03:35<603:27:34, 36.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8742), tensor(12.2619), tensor(11.6852, grad_fn=<AddBackward0>), tensor(11.3049, grad_fn=<AddBackward0>), tensor(11.0130, grad_fn=<AddBackward0>), tensor(10.6884, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.3360533714294434\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 7/60000 [03:37<431:50:37, 25.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8970), tensor(12.2559), tensor(11.7217, grad_fn=<AddBackward0>), tensor(11.3731, grad_fn=<AddBackward0>), tensor(11.0388, grad_fn=<AddBackward0>), tensor(10.8662, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.3582772016525269\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 8/60000 [03:39<312:08:48, 18.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8883), tensor(12.1751), tensor(11.5437, grad_fn=<AddBackward0>), tensor(11.1568, grad_fn=<AddBackward0>), tensor(10.7346, grad_fn=<AddBackward0>), tensor(10.3971, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.299637794494629\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 9/60000 [03:41<229:31:51, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8808), tensor(12.0941), tensor(11.4841, grad_fn=<AddBackward0>), tensor(11.0384, grad_fn=<AddBackward0>), tensor(10.7203, grad_fn=<AddBackward0>), tensor(10.5409, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.3176085948944092\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                           | 10/60000 [03:43<170:46:21, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8738), tensor(12.0745), tensor(11.4202, grad_fn=<AddBackward0>), tensor(10.9645, grad_fn=<AddBackward0>), tensor(10.6279, grad_fn=<AddBackward0>), tensor(10.3738, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.2967206239700317\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                           | 11/60000 [03:45<130:01:32,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8779), tensor(12.3439), tensor(11.8744, grad_fn=<AddBackward0>), tensor(11.5872, grad_fn=<AddBackward0>), tensor(11.2649, grad_fn=<AddBackward0>), tensor(11.1187, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.3898402452468872\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                           | 12/60000 [03:47<100:58:06,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8912), tensor(12.0773), tensor(11.4105, grad_fn=<AddBackward0>), tensor(11.0470, grad_fn=<AddBackward0>), tensor(10.7277, grad_fn=<AddBackward0>), tensor(10.5958, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.324477195739746\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 13/60000 [03:49<80:44:37,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8821), tensor(12.1198), tensor(11.5342, grad_fn=<AddBackward0>), tensor(11.1156, grad_fn=<AddBackward0>), tensor(10.8480, grad_fn=<AddBackward0>), tensor(10.6135, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.3266873359680176\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 14/60000 [03:52<68:06:43,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8707), tensor(11.9712), tensor(11.3064, grad_fn=<AddBackward0>), tensor(10.8981, grad_fn=<AddBackward0>), tensor(10.5977, grad_fn=<AddBackward0>), tensor(10.4323, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.3040434122085571\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 15/60000 [03:55<64:33:56,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8899), tensor(12.2228), tensor(11.7585, grad_fn=<AddBackward0>), tensor(11.4469, grad_fn=<AddBackward0>), tensor(11.2322, grad_fn=<AddBackward0>), tensor(11.1385, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.3923156261444092\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 16/60000 [03:58<59:32:26,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8710), tensor(12.0454), tensor(11.4432, grad_fn=<AddBackward0>), tensor(10.9726, grad_fn=<AddBackward0>), tensor(10.6707, grad_fn=<AddBackward0>), tensor(10.5504, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.3187943696975708\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 17/60000 [04:01<56:28:29,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8658), tensor(11.8267), tensor(11.0232, grad_fn=<AddBackward0>), tensor(10.4618, grad_fn=<AddBackward0>), tensor(10.0203, grad_fn=<AddBackward0>), tensor(9.7782, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.2222734689712524\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 18/60000 [04:04<57:17:21,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8845), tensor(11.5458), tensor(10.5631, grad_fn=<AddBackward0>), tensor(9.9092, grad_fn=<AddBackward0>), tensor(9.5268, grad_fn=<AddBackward0>), tensor(9.2264, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.1533045768737793\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 19/60000 [04:07<54:06:51,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8777), tensor(11.9274), tensor(11.2393, grad_fn=<AddBackward0>), tensor(10.7175, grad_fn=<AddBackward0>), tensor(10.3604, grad_fn=<AddBackward0>), tensor(10.1845, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.2730660438537598\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 20/60000 [04:10<51:17:59,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8916), tensor(11.8882), tensor(11.1249, grad_fn=<AddBackward0>), tensor(10.5509, grad_fn=<AddBackward0>), tensor(10.2032, grad_fn=<AddBackward0>), tensor(9.9723, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.24653959274292\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 21/60000 [04:13<49:27:35,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8648), tensor(11.7372), tensor(10.9569, grad_fn=<AddBackward0>), tensor(10.4905, grad_fn=<AddBackward0>), tensor(10.1708, grad_fn=<AddBackward0>), tensor(10.0414, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.2551701068878174\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 22/60000 [04:16<51:02:10,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8872), tensor(11.7657), tensor(10.9234, grad_fn=<AddBackward0>), tensor(10.3152, grad_fn=<AddBackward0>), tensor(9.9200, grad_fn=<AddBackward0>), tensor(9.6577, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.2072089910507202\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 23/60000 [04:19<49:28:28,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.9070), tensor(11.8710), tensor(11.1213, grad_fn=<AddBackward0>), tensor(10.6254, grad_fn=<AddBackward0>), tensor(10.2589, grad_fn=<AddBackward0>), tensor(9.9976, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.2496942281723022\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 24/60000 [04:21<47:07:17,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8631), tensor(12.0412), tensor(11.4011, grad_fn=<AddBackward0>), tensor(10.9874, grad_fn=<AddBackward0>), tensor(10.7213, grad_fn=<AddBackward0>), tensor(10.5906, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.3238290548324585\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 25/60000 [04:24<46:06:52,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8917), tensor(11.5949), tensor(10.6771, grad_fn=<AddBackward0>), tensor(10.0662, grad_fn=<AddBackward0>), tensor(9.6673, grad_fn=<AddBackward0>), tensor(9.3729, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.1716153621673584\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 26/60000 [04:26<44:19:47,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8647), tensor(11.5765), tensor(10.6041, grad_fn=<AddBackward0>), tensor(9.9499, grad_fn=<AddBackward0>), tensor(9.5196, grad_fn=<AddBackward0>), tensor(9.2765, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.1595637798309326\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 27/60000 [04:29<43:07:08,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8337), tensor(11.7113), tensor(10.8902, grad_fn=<AddBackward0>), tensor(10.3288, grad_fn=<AddBackward0>), tensor(9.9904, grad_fn=<AddBackward0>), tensor(9.8265, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.2283138036727905\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 28/60000 [04:31<42:32:15,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8975), tensor(11.7654), tensor(10.8672, grad_fn=<AddBackward0>), tensor(10.2767, grad_fn=<AddBackward0>), tensor(9.8456, grad_fn=<AddBackward0>), tensor(9.6311, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.2038885354995728\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 29/60000 [04:34<42:47:50,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.9052), tensor(11.5890), tensor(10.6445, grad_fn=<AddBackward0>), tensor(10.0151, grad_fn=<AddBackward0>), tensor(9.6081, grad_fn=<AddBackward0>), tensor(9.3851, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.1731383800506592\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 30/60000 [04:36<41:58:54,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.9066), tensor(11.6574), tensor(10.8196, grad_fn=<AddBackward0>), tensor(10.2368, grad_fn=<AddBackward0>), tensor(9.8603, grad_fn=<AddBackward0>), tensor(9.6538, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.2067267894744873\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 31/60000 [04:39<43:29:54,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.9127), tensor(11.5392), tensor(10.6422, grad_fn=<AddBackward0>), tensor(10.0532, grad_fn=<AddBackward0>), tensor(9.6546, grad_fn=<AddBackward0>), tensor(9.4442, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.180525302886963\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 32/60000 [04:41<42:52:47,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.9008), tensor(11.5918), tensor(10.7119, grad_fn=<AddBackward0>), tensor(10.0953, grad_fn=<AddBackward0>), tensor(9.7555, grad_fn=<AddBackward0>), tensor(9.5720, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.1964945793151855\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 33/60000 [04:44<42:26:03,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8667), tensor(11.5406), tensor(10.6645, grad_fn=<AddBackward0>), tensor(10.1342, grad_fn=<AddBackward0>), tensor(9.8594, grad_fn=<AddBackward0>), tensor(9.7053, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.213165283203125\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 34/60000 [04:46<42:04:38,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8761), tensor(11.5584), tensor(10.6445, grad_fn=<AddBackward0>), tensor(10.0813, grad_fn=<AddBackward0>), tensor(9.7397, grad_fn=<AddBackward0>), tensor(9.5369, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.1921186447143555\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 35/60000 [04:49<41:38:38,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8848), tensor(11.7135), tensor(10.9930, grad_fn=<AddBackward0>), tensor(10.5202, grad_fn=<AddBackward0>), tensor(10.3284, grad_fn=<AddBackward0>), tensor(10.1858, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.2732274532318115\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 36/60000 [04:51<41:10:06,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8998), tensor(11.2972), tensor(10.2872, grad_fn=<AddBackward0>), tensor(9.6670, grad_fn=<AddBackward0>), tensor(9.2740, grad_fn=<AddBackward0>), tensor(9.0565, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.1320602893829346\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 37/60000 [04:54<40:47:22,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8859), tensor(11.5111), tensor(10.6537, grad_fn=<AddBackward0>), tensor(10.1595, grad_fn=<AddBackward0>), tensor(9.8568, grad_fn=<AddBackward0>), tensor(9.6798, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.2099714279174805\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 38/60000 [04:56<41:58:23,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8726), tensor(11.1066), tensor(9.9995, grad_fn=<AddBackward0>), tensor(9.2915, grad_fn=<AddBackward0>), tensor(8.8131, grad_fn=<AddBackward0>), tensor(8.5687, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.071083664894104\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 39/60000 [04:59<41:29:38,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8770), tensor(11.3606), tensor(10.3524, grad_fn=<AddBackward0>), tensor(9.7011, grad_fn=<AddBackward0>), tensor(9.3035, grad_fn=<AddBackward0>), tensor(9.0731, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.134136438369751\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 40/60000 [05:01<41:22:52,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8793), tensor(11.1692), tensor(10.0752, grad_fn=<AddBackward0>), tensor(9.4399, grad_fn=<AddBackward0>), tensor(9.0189, grad_fn=<AddBackward0>), tensor(8.8018, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.100219964981079\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 41/60000 [05:05<47:10:00,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8965), tensor(11.4964), tensor(10.5954, grad_fn=<AddBackward0>), tensor(10.0672, grad_fn=<AddBackward0>), tensor(9.7818, grad_fn=<AddBackward0>), tensor(9.5876, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.1984553337097168\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 42/60000 [05:07<45:56:15,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8564), tensor(11.3999), tensor(10.4862, grad_fn=<AddBackward0>), tensor(9.9562, grad_fn=<AddBackward0>), tensor(9.6526, grad_fn=<AddBackward0>), tensor(9.4850, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.1856274604797363\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 43/60000 [05:10<44:26:18,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8636), tensor(11.2284), tensor(10.2751, grad_fn=<AddBackward0>), tensor(9.7480, grad_fn=<AddBackward0>), tensor(9.4447, grad_fn=<AddBackward0>), tensor(9.2211, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.1526345014572144\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 44/60000 [05:13<44:53:49,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8597), tensor(11.4205), tensor(10.4648, grad_fn=<AddBackward0>), tensor(9.8282, grad_fn=<AddBackward0>), tensor(9.3884, grad_fn=<AddBackward0>), tensor(9.1887, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.14858877658844\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 45/60000 [05:16<46:43:42,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8373), tensor(10.9849), tensor(9.8100, grad_fn=<AddBackward0>), tensor(9.1261, grad_fn=<AddBackward0>), tensor(8.6666, grad_fn=<AddBackward0>), tensor(8.4487, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.0560821294784546\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 46/60000 [05:18<45:17:19,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8506), tensor(11.3567), tensor(10.4338, grad_fn=<AddBackward0>), tensor(9.9143, grad_fn=<AddBackward0>), tensor(9.6399, grad_fn=<AddBackward0>), tensor(9.4946, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.1868281364440918\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 47/60000 [05:21<45:08:05,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8450), tensor(10.9930), tensor(9.9088, grad_fn=<AddBackward0>), tensor(9.2934, grad_fn=<AddBackward0>), tensor(8.9182, grad_fn=<AddBackward0>), tensor(8.7453, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.0931596755981445\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 48/60000 [05:24<45:29:12,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8568), tensor(10.9958), tensor(9.9462, grad_fn=<AddBackward0>), tensor(9.3348, grad_fn=<AddBackward0>), tensor(8.9912, grad_fn=<AddBackward0>), tensor(8.7738, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.0967226028442383\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 49/60000 [05:27<46:48:03,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8953), tensor(11.1334), tensor(10.0132, grad_fn=<AddBackward0>), tensor(9.3398, grad_fn=<AddBackward0>), tensor(8.9260, grad_fn=<AddBackward0>), tensor(8.6788, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.0848559141159058\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 50/60000 [05:29<46:50:42,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n",
      "loss_list_qry:[tensor(12.8848), tensor(11.2074), tensor(10.2976, grad_fn=<AddBackward0>), tensor(9.7680, grad_fn=<AddBackward0>), tensor(9.4858, grad_fn=<AddBackward0>), tensor(9.3136, grad_fn=<AddBackward0>)]\n",
      "loss_qry:1.16420316696167\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 51/60000 [05:33<48:15:25,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task num:8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 51/60000 [05:33<108:55:38,  6.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-18c5149dee1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                  \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_qry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                  \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_qry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0maccs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_spt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_spt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_qry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_qry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-5e05bef874f3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_spt, y_spt, x_qry, y_qry)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_spt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfast_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_spt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfast_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                 \u001b[0mtuples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfast_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mfast_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_lr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    190\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[0;32m    191\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m         inputs, allow_unused)\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## omniglot\n",
    "\n",
    "import time\n",
    "#device = torch.device('cuda')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "meta = MetaLearner().to(device)\n",
    "\n",
    "epochs = 60000\n",
    "for step in tqdm(range(epochs)):\n",
    "    start = time.time()\n",
    "    x_spt, y_spt, x_qry, y_qry = next('train')\n",
    "    x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device),\\\n",
    "                                 torch.from_numpy(y_spt).to(device),\\\n",
    "                                 torch.from_numpy(x_qry).to(device),\\\n",
    "                                 torch.from_numpy(y_qry).to(device)\n",
    "    accs,loss = meta(x_spt, y_spt.long(), x_qry, y_qry.long())\n",
    "    end = time.time()\n",
    "    if step % 100 == 0:\n",
    "        print(\"epoch:\" ,step)\n",
    "        print(accs)\n",
    "        print(loss)\n",
    "        \n",
    "    if step % 1000 == 0:\n",
    "        accs = []\n",
    "        for _ in range(1000//task_num):\n",
    "            # db_train.next('test')\n",
    "            x_spt, y_spt, x_qry, y_qry = next('test')\n",
    "            x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device),\\\n",
    "                                         torch.from_numpy(y_spt).to(device),\\\n",
    "                                         torch.from_numpy(x_qry).to(device),\\\n",
    "                                         torch.from_numpy(y_qry).to(device)\n",
    "\n",
    "            \n",
    "            for x_spt_one, y_spt_one, x_qry_one, y_qry_one in zip(x_spt, y_spt, x_qry, y_qry):\n",
    "                test_acc = meta.finetunning(x_spt_one, y_spt_one.long(), x_qry_one, y_qry_one.long())\n",
    "                accs.append(test_acc)\n",
    "        print('在mean process之前：',np.array(accs).shape)\n",
    "        accs = np.array(accs).mean(axis=0).astype(np.float16)\n",
    "        print('测试集准确率:',accs)\n",
    "                                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
