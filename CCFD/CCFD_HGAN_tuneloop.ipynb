{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:06:03.170001Z",
     "start_time": "2020-08-29T12:05:59.865484Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "J3BHjbqDAeK_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#AUCROC,AUCPR,precision,recall,f1-score\n",
    "from sklearn.metrics import roc_curve,auc,average_precision_score,precision_score,recall_score,f1_score\n",
    "\n",
    "#gridsearch/randomsearch\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "#visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:06:04.623596Z",
     "start_time": "2020-08-29T12:06:03.177981Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-ARYrELmA6cj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Subset,DataLoader,TensorDataset\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:06:04.655107Z",
     "start_time": "2020-08-29T12:06:04.628553Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "J_P_1SmrH8Gv",
    "outputId": "88cde7d4-747f-4d6c-f981-87a9de0ca4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is off\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  is_cuda=True\n",
    "  print('GPU is on')\n",
    "else:\n",
    "  is_cuda=False\n",
    "  print('GPU is off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:06:04.671011Z",
     "start_time": "2020-08-29T12:06:04.660043Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lp22xWtgqQAn"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:06:04.702926Z",
     "start_time": "2020-08-29T12:06:04.675998Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xywA-mEoH8G0"
   },
   "outputs": [],
   "source": [
    "def data_prepare(batch_size,contam_ratio,hybrid,seed):\n",
    "    set_seed(seed)\n",
    "    data = pd.read_csv(\"D:/Jiang/Research_Anomaly Detection/Important_Credit Card Fraud Detection (CCFD)/creditcardfraud/creditcard.csv\")\n",
    "    #change 0,1 label to 1,-1\n",
    "    data.loc[data['Class']==1,'Class'] = -1\n",
    "    data.loc[data['Class']==0,'Class'] = 1\n",
    "\n",
    "    X = data.drop(['Time','Class'], axis=1)\n",
    "    y = data[\"Class\"].values\n",
    "\n",
    "    #split the data to training, validation and testing data (50%,20%,30%)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,shuffle=False)\n",
    "    X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=2/7,shuffle=False)\n",
    "\n",
    "    #the known positive samples before contaminating\n",
    "    known_pos_entire = sum(y_train == -1)\n",
    "    #Minmax\n",
    "    scaler=MinMaxScaler().fit(X_train)\n",
    "\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    index_contam = np.arange(len(y_train))[y_train == -1]\n",
    "    index_contam = np.random.choice(index_contam,int(contam_ratio*len(index_contam)),replace = False)\n",
    "\n",
    "    y_train[index_contam] = 1\n",
    "    #the known positive samples after contaminating\n",
    "    known_pos_sub = sum(y_train == -1)\n",
    "\n",
    "    print(f'The left true(known) positive samples in the training set:{known_pos_sub}/{known_pos_entire}\\n')\n",
    "    #normal training tensor and dataloader\n",
    "    #none-hybrid model only use \"normal\" samples in the training phase, which could be contaminated\n",
    "    if not hybrid:\n",
    "        index_subset = np.arange(len(y_train))[y_train == 1]\n",
    "    else:\n",
    "        index_subset = np.arange(len(y_train))\n",
    "    \n",
    "\n",
    "    #transform numpy to pytorch tensor\n",
    "    train_tensor = TensorDataset(torch.from_numpy(X_train).float(),torch.tensor(y_train))\n",
    "    train_tensor = Subset(train_tensor,index_subset)\n",
    "    #fitting by batches (using dataloader), there exists randomness when shuffle=True***\n",
    "    train_loader=DataLoader(train_tensor,batch_size=batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    #validation set\n",
    "    X_val_tensor = torch.from_numpy(X_val).float()\n",
    "    #testing set\n",
    "    X_test_tensor=torch.from_numpy(X_test).float()\n",
    "    \n",
    "    return train_loader,X_val_tensor,y_val,X_test_tensor,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:06:04.734842Z",
     "start_time": "2020-08-29T12:06:04.707914Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4h3QB1sNH8G2"
   },
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "  def __init__(self,input_size,act_fun):\n",
    "    super(generator,self).__init__()\n",
    "\n",
    "    self.encoder_1=nn.Sequential(\n",
    "      nn.Linear(input_size,input_size//4),\n",
    "      act_fun,\n",
    "      )\n",
    "    \n",
    "    self.decoder_1=nn.Sequential(\n",
    "      nn.Linear(input_size//4,input_size),\n",
    "      )\n",
    "\n",
    "    self.encoder_2=nn.Sequential(\n",
    "      nn.Linear(input_size,input_size//4),\n",
    "      act_fun,\n",
    "      )\n",
    "\n",
    "  def forward(self,input):\n",
    "    z=self.encoder_1(input)\n",
    "    X_hat=self.decoder_1(z)\n",
    "    z_hat=self.encoder_2(X_hat)\n",
    "\n",
    "    return z,X_hat,z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:06:04.766756Z",
     "start_time": "2020-08-29T12:06:04.741825Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oJVorzdrH8G4"
   },
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "  def __init__(self,input_size,act_fun):\n",
    "    super(discriminator,self).__init__()\n",
    "\n",
    "    self.encoder=nn.Sequential(\n",
    "        nn.Linear(input_size,input_size//4),\n",
    "        act_fun,\n",
    "        )\n",
    "\n",
    "    self.classifier=nn.Sequential(\n",
    "        nn.Linear(input_size//4,1),\n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "  def forward(self,input):\n",
    "    latent_vector=self.encoder(input)\n",
    "    output=self.classifier(latent_vector)\n",
    "\n",
    "    return latent_vector,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:06:04.844840Z",
     "start_time": "2020-08-29T12:06:04.803041Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "77CBVelfH8G8"
   },
   "outputs": [],
   "source": [
    "def fit(dataloader,net_generator,net_discriminator,eta,epochs,batch_size,print_loss):\n",
    "  L1_criterion = nn.L1Loss(reduction='none')\n",
    "  L2_criterion = nn.MSELoss(reduction='none')\n",
    "  BCE_criterion = nn.BCELoss(reduction='mean')\n",
    "\n",
    "  if is_cuda:\n",
    "    L1_criterion.cuda()\n",
    "    L2_criterion.cuda()\n",
    "    BCE_criterion.cuda()\n",
    "\n",
    "  loss_D_list = []\n",
    "  loss_G_list = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    for i,data in enumerate(dataloader):\n",
    "      #y_aclabel means the acquired label information (which may be contaminated)\n",
    "      X,y_aclabel = data\n",
    "      y_real = torch.FloatTensor(batch_size).fill_(0)#real label=0,size=batch_size\n",
    "      y_fake = torch.FloatTensor(batch_size).fill_(1)#fake label=1,size=batch_size\n",
    "\n",
    "      if is_cuda:\n",
    "        X = X.cuda()\n",
    "        y_aclabel = y_aclabel.cuda()\n",
    "        y_real = y_real.cuda()\n",
    "        y_fake = y_fake.cuda()\n",
    "\n",
    "      X = Variable(X)\n",
    "      y_aclabel = Variable(y_aclabel)\n",
    "      y_real = Variable(y_real)\n",
    "      y_fake = Variable(y_fake)\n",
    "\n",
    "      #zero grad for discriminator\n",
    "      net_discriminator.zero_grad()\n",
    "      #training the discriminator with real sample\n",
    "      _,output = net_discriminator(X)\n",
    "      loss_D_real = BCE_criterion(output.view(-1),y_real)\n",
    "\n",
    "      #training the discriminator with fake sample\n",
    "      _,X_hat,_ = net_generator(X)\n",
    "      _,output = net_discriminator(X_hat)\n",
    "\n",
    "      loss_D_fake = BCE_criterion(output.view(-1),y_fake)\n",
    "\n",
    "      #entire loss in discriminator\n",
    "      loss_D = (loss_D_real+loss_D_fake)/2\n",
    "      \n",
    "      loss_D.backward()\n",
    "      optimizer_D.step()\n",
    "\n",
    "      #training the generator based on the result from the discriminator\n",
    "      net_generator.zero_grad()\n",
    "\n",
    "      z,X_hat,z_hat = net_generator(X)\n",
    "\n",
    "      #latent loss\n",
    "      feature_real,_ = net_discriminator(X)\n",
    "      feature_fake,_ = net_discriminator(X_hat)\n",
    "\n",
    "      loss_G_latent = torch.mean(L2_criterion(feature_fake,feature_real),1)\n",
    "      \n",
    "      #contexutal loss\n",
    "      loss_G_contextual = torch.mean(L1_criterion(X,X_hat),1)\n",
    "      #entire loss in generator\n",
    "\n",
    "      #encoder loss\n",
    "      loss_G_encoder = torch.mean(L1_criterion(z,z_hat),1)\n",
    "      \n",
    "      loss_G = (loss_G_latent + loss_G_contextual + loss_G_encoder)/3\n",
    "      \n",
    "      loss_G_normal = (loss_G[y_aclabel == 1])\n",
    "      loss_G_anomaly = pow(loss_G[y_aclabel == -1],-1)\n",
    "      \n",
    "      if loss_G_anomaly.size(0)>0:\n",
    "        loss_G = (1-eta)*(sum(loss_G_normal)/loss_G_normal.size(0))+\\\n",
    "                eta*(sum(loss_G_anomaly)/loss_G_anomaly.size(0))\n",
    "      else:\n",
    "        loss_G = sum(loss_G_normal)/loss_G_normal.size(0)\n",
    "      \n",
    "      loss_G.backward()\n",
    "      optimizer_G.step()\n",
    "\n",
    "      if (i%50 == 0) & print_loss:\n",
    "        print('[%d/%d] [%d/%d] Loss D: %.4f / Loss G: %.4f' % (epoch+1,epochs,i,len(dataloader),loss_D,loss_G))\n",
    "\n",
    "      loss_D_list.append(loss_D)\n",
    "      loss_G_list.append(loss_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:06:04.860314Z",
     "start_time": "2020-08-29T12:06:04.850340Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "iuNcHJ0jH8G-"
   },
   "outputs": [],
   "source": [
    "def evaluation(data_tensor,model):\n",
    "  if is_cuda:\n",
    "    data_tensor = data_tensor.cuda()\n",
    "    \n",
    "  z,_,z_hat = model(data_tensor)\n",
    "\n",
    "  L1_criterion = nn.L1Loss(reduction='none')\n",
    "  score = L1_criterion(z,z_hat)\n",
    "  score = torch.sum(score,dim=1).cpu().detach().numpy()\n",
    "    \n",
    "  return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPkCKyDfH8HA"
   },
   "source": [
    "Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWmxDA8UUY3G"
   },
   "outputs": [],
   "source": [
    "seed_pool = [1,2,3,4,5]\n",
    "anomaly_ratio_pool = [0.001,0.002,0.003]\n",
    "contam_ratio_pool = [1.0,0.98,0.5,0.0]\n",
    "#hybrid hyper-parameter\n",
    "eta = 0.5\n",
    "#random search size\n",
    "search_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:06:04.891231Z",
     "start_time": "2020-08-29T12:06:04.866301Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5nlvzFGCH8HA"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "hyper_act_fun = [nn.Tanh(),nn.LeakyReLU()]\n",
    "hyper_lr = [1e-2,1e-3,1e-4]\n",
    "hyper_mom = [0.5,0.7,0.9]\n",
    "\n",
    "hyper_list_entire = list(product(hyper_act_fun,hyper_lr,hyper_mom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_DOiFCZwRhH"
   },
   "outputs": [],
   "source": [
    "def random_search(hyper_list_entire,search_size,seed):\n",
    "  if search_size < len(hyper_list_entire):\n",
    "    set_seed(seed)\n",
    "    index = np.random.choice(np.arange(len(hyper_list_entire)),search_size,replace = False)\n",
    "\n",
    "    hyper_list = []\n",
    "    for i in index:\n",
    "      hyper_list.append(hyper_list_entire[i])\n",
    "  else:\n",
    "    hyper_list = hyper_list_entire\n",
    "\n",
    "  return hyper_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-29T12:05:59.853Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "1Q-awzakH8HF",
    "outputId": "c3a2cdf9-08e6-44c0-f59f-9504e8b92857"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.706\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8118\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8108\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.9)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.3386\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.9)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8084\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.7783\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.0001, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.4003\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8106\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8311\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.7983\n",
      "******************************\n",
      "\n",
      "\n",
      "The best hyper-parameters are: (Tanh(), 0.01, 0.7)\n",
      "\n",
      "\n",
      "Testing Phrase......\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "\n",
      "\n",
      "Precision: 83.72\n",
      "Recall: 66.67\n",
      "F1-score: 74.23\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 51.46\n",
      "Recall: 81.48\n",
      "F1-score: 63.08\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 35.02\n",
      "Recall: 83.33\n",
      "F1-score: 49.32\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████████████                                                                | 1/5 [29:26<1:57:46, 1766.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8101\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8048\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.0001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.7879\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.71\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8146\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.9)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8147\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8156\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.6029\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8177\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.7907\n",
      "******************************\n",
      "\n",
      "\n",
      "The best hyper-parameters are: (LeakyReLU(negative_slope=0.01), 0.001, 0.5)\n",
      "\n",
      "\n",
      "Testing Phrase......\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "\n",
      "\n",
      "Precision: 86.05\n",
      "Recall: 68.52\n",
      "F1-score: 76.29\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 49.71\n",
      "Recall: 78.7\n",
      "F1-score: 60.93\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 34.24\n",
      "Recall: 81.48\n",
      "F1-score: 48.22\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|███████████████████████████████▏                                              | 2/5 [1:30:24<1:56:42, 2334.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.9)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.3988\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.9)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.07208\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.001547\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.9)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8127\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8107\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8104\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.7668\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.7966\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8182\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.001057\n",
      "******************************\n",
      "\n",
      "\n",
      "The best hyper-parameters are: (LeakyReLU(negative_slope=0.01), 0.001, 0.5)\n",
      "\n",
      "\n",
      "Testing Phrase......\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "\n",
      "\n",
      "Precision: 88.37\n",
      "Recall: 70.37\n",
      "F1-score: 78.35\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 50.29\n",
      "Recall: 79.63\n",
      "F1-score: 61.65\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 34.24\n",
      "Recall: 81.48\n",
      "F1-score: 48.22\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████████████████████████████████████████████▊                               | 3/5 [2:29:38<1:29:59, 2699.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.6613\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8136\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.0001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.786\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.9)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8071\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8088\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.0001, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.7032\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8162\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8174\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8107\n",
      "******************************\n",
      "\n",
      "\n",
      "The best hyper-parameters are: (LeakyReLU(negative_slope=0.01), 0.01, 0.7)\n",
      "\n",
      "\n",
      "Testing Phrase......\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "\n",
      "\n",
      "Precision: 82.56\n",
      "Recall: 65.74\n",
      "F1-score: 73.2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 51.46\n",
      "Recall: 81.48\n",
      "F1-score: 63.08\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 34.63\n",
      "Recall: 82.41\n",
      "F1-score: 48.77\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████████████████████████████████████████████████████████████                | 4/5 [3:33:14<50:34, 3034.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.9)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8184\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8317\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8194\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.9)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8283\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.9)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.0001, 0.9)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8222\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8179\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8028\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8151\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.9)\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.8107\n",
      "******************************\n",
      "\n",
      "\n",
      "The best hyper-parameters are: (Tanh(), 0.01, 0.7)\n",
      "\n",
      "\n",
      "Testing Phrase......\n",
      "The left true(known) positive samples in the training set:269/269\n",
      "\n",
      "\n",
      "\n",
      "Precision: 90.7\n",
      "Recall: 72.22\n",
      "F1-score: 80.41\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 50.88\n",
      "Recall: 80.56\n",
      "F1-score: 62.37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 35.02\n",
      "Recall: 83.33\n",
      "F1-score: 49.32\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [4:29:33<00:00, 16173.45s/it]\n"
     ]
    }
   ],
   "source": [
    "for contam_ratio in tqdm(contam_ratio_pool):\n",
    "  df_result = pd.DataFrame(data = None,index = ['AUCPR'] + anomaly_ratio_pool,columns = seed_pool) \n",
    "\n",
    "  for seed in tqdm(seed_pool):\n",
    "    #############################################seleting the best hyper-parameters in validation set#############################################\n",
    "    metric_value_list = list()\n",
    "    hyper_list = random_search(hyper_list_entire,search_size,seed)\n",
    "    for i in range(len(hyper_list)):\n",
    "      try:\n",
    "        print(f'Finding Optimal Hyper-parameters......Current Candidates: {hyper_list[i]}')\n",
    "        act_fun,lr,mom = hyper_list[i]\n",
    "        #data\n",
    "        train_loader,X_val_tensor,y_val,_,_ = data_prepare(batch_size,contam_ratio,hybrid = True,seed = seed)\n",
    "        #model initialization\n",
    "        set_seed(seed)\n",
    "        net_generator=generator(input_size = X_val_tensor.size(1),act_fun = act_fun)\n",
    "        net_discriminator=discriminator(input_size = X_val_tensor.size(1),act_fun = act_fun)\n",
    "\n",
    "        if is_cuda:\n",
    "          net_generator.cuda()\n",
    "          net_discriminator.cuda()\n",
    "\n",
    "        optimizer_G = torch.optim.SGD(net_generator.parameters(),lr = lr,momentum = mom)\n",
    "        optimizer_D = torch.optim.SGD(net_discriminator.parameters(),lr = lr,momentum = mom)\n",
    "        #fitting\n",
    "        fit(dataloader = train_loader,net_generator = net_generator,net_discriminator = net_discriminator,\n",
    "          eta = eta,epochs = epochs,batch_size = batch_size,print_loss = False)\n",
    "        \n",
    "        #evaluation\n",
    "        score = evaluation(data_tensor = X_val_tensor,model = net_generator)\n",
    "        metric_value = average_precision_score(y_true = y_val,y_score = score,pos_label = -1)\n",
    "        metric_value_list.append(metric_value)\n",
    "\n",
    "        print(f'The metric value corresponded to the hyper-parameters is :{metric_value:{.4}}')\n",
    "        print('******************************')\n",
    "        print('\\n')\n",
    "      except:\n",
    "        #keep the right index\n",
    "        metric_value_list.append(0)\n",
    "        pass\n",
    "      continue\n",
    "\n",
    "    best_hyper_params=hyper_list[metric_value_list.index(max(metric_value_list))]\n",
    "    print(f'The best hyper-parameters are: {best_hyper_params}')\n",
    "    print('\\n')\n",
    "    ###################################################################testing#########################################################################\n",
    "    print('Testing Phrase......')\n",
    "    act_fun,lr,mom = best_hyper_params\n",
    "\n",
    "    #data\n",
    "    train_loader,_,_,X_test_tensor,y_test = data_prepare(batch_size,contam_ratio,hybrid = True,seed = seed)\n",
    "    \n",
    "    #model initialization, there exists randomness because of weight initialization***\n",
    "    set_seed(seed)\n",
    "    net_generator=generator(input_size = X_test_tensor.size(1),act_fun = act_fun)\n",
    "    net_discriminator=discriminator(input_size = X_test_tensor.size(1),act_fun = act_fun)\n",
    "\n",
    "    if is_cuda:\n",
    "      net_generator.cuda()\n",
    "      net_discriminator.cuda()\n",
    "\n",
    "    optimizer_G = torch.optim.SGD(net_generator.parameters(),lr = lr,momentum = mom)\n",
    "    optimizer_D = torch.optim.SGD(net_discriminator.parameters(),lr = lr,momentum = mom)\n",
    "    #fitting\n",
    "    fit(dataloader = train_loader,net_generator = net_generator,net_discriminator = net_discriminator,\n",
    "      eta = eta,epochs = epochs,batch_size = batch_size,print_loss = False)\n",
    "    #evaluation\n",
    "    score = evaluation(data_tensor = X_test_tensor,model = net_generator)\n",
    "    \n",
    "    #store the result\n",
    "    #AUCPR\n",
    "    df_result.loc['AUCPR',seed] = average_precision_score(y_true = y_test,y_score = score,pos_label = -1)\n",
    "    #F1\n",
    "    for anomaly_ratio in anomaly_ratio_pool:\n",
    "        threshold = score[np.argsort(-score)][int(anomaly_ratio*len(score))]\n",
    "        \n",
    "        y_pred = np.ones(len(score))\n",
    "        y_pred[score >= threshold] = -1\n",
    "        \n",
    "        print('\\n')\n",
    "        print(f'Precision: {round(precision_score(y_pred = y_pred, y_true = y_test, pos_label= -1)*100,2)}')\n",
    "        print(f'Recall: {round(recall_score(y_pred = y_pred, y_true = y_test, pos_label= -1)*100,2)}')\n",
    "        print(f'F1-score: {round(f1_score(y_pred = y_pred, y_true = y_test, pos_label= -1)*100,2)}')\n",
    "        print('\\n')\n",
    "\n",
    "        df_result.loc[anomaly_ratio,seed] = f1_score(y_pred = y_pred,y_true = y_test,pos_label = -1) \n",
    "  \n",
    "  #mean & sd\n",
    "  df_result['mean'] = np.mean(df_result.loc[:,seed_pool],axis = 1)\n",
    "  df_result['std'] = np.std(df_result.loc[:,seed_pool],axis = 1)\n",
    "  df_result = round(df_result.astype('float64')*100,2)\n",
    "\n",
    "  file_path = 'D:/Jiang/Research_Anomaly Detection/Important_Credit Card Fraud Detection (CCFD)/Hyprid Semi-supervised/result/' + 'CCFD_HGAN_' + str(contam_ratio) + '.csv'\n",
    "  df_result.to_csv(file_path,index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CCFD_HGAN_tuneloop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
