{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "cv_gan_aeplus_hybrid_loop",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvUwidZYEFuc",
        "trusted": true,
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy.io\n",
        "\n",
        "#split dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#accuracy,AUCROC,precision,recall,f1-score,AUCPR\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#gridsearch/randomsearch\n",
        "from itertools import product\n",
        "from tqdm import tqdm\n",
        "\n",
        "#visualize results\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ6WYUvgEOsX",
        "trusted": true,
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Subset,DataLoader,TensorDataset\n",
        "from torchvision import datasets,transforms\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JuUsEkemUC0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_seed(seed):\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46w-n5jpG0wY",
        "trusted": true,
        "scrolled": false,
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f6465c9-2d92-4018-c0a0-215017ae0234"
      },
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  is_cuda=True\n",
        "  print('GPU is on')\n",
        "else:\n",
        "  is_cuda=False\n",
        "  print('GPU is off')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFQTrdLlwXDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_prepare(dataset,batch_size,target_num,contam_ratio,seed,hybrid): \n",
        "  #set seed for reproductive results\n",
        "  set_seed(seed)\n",
        "\n",
        "  if dataset == 'MNIST':\n",
        "    transformation = transforms.Compose([transforms.Resize(img_size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    #entire training tensor\n",
        "    train_tensor = torchvision.datasets.MNIST('data/',train=True,transform=transformation,download=True)\n",
        "    #testing tensor\n",
        "    test_tensor = torchvision.datasets.MNIST('data/',train=False,transform=transformation,download=True)\n",
        "\n",
        "  elif dataset == 'FashionMNIST':\n",
        "    transformation = transforms.Compose([transforms.Resize(img_size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.5,), (0.5,))])\n",
        "    #entire training tensor\n",
        "    train_tensor = torchvision.datasets.FashionMNIST('data/',train=True,transform=transformation,download=True)\n",
        "    #testing tensor\n",
        "    test_tensor = torchvision.datasets.FashionMNIST('data/',train=False,transform=transformation,download=True)\n",
        "\n",
        "  elif dataset == 'CIFAR10':\n",
        "    transformation = transforms.Compose([transforms.Resize(img_size),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
        "    #entire training tensor\n",
        "    train_tensor = torchvision.datasets.CIFAR10('data/',train=True,transform=transformation,download=True)\n",
        "    #testing tensor\n",
        "    test_tensor = torchvision.datasets.CIFAR10('data/',train=False,transform=transformation,download=True)\n",
        "\n",
        "  train_X = torch.empty([len(train_tensor),train_tensor[0][0].size(0),img_size,img_size])\n",
        "  train_y = []\n",
        "\n",
        "  if mode == 'leave_one_anomaly':\n",
        "    #only using normal samples to train the model\n",
        "    for i,data in enumerate(train_tensor):\n",
        "      X,y = data\n",
        "      train_X[i] = X\n",
        "      if y != target_num:\n",
        "        train_y.append(1)\n",
        "      else:\n",
        "        train_y.append(-1)\n",
        "\n",
        "  elif mode == 'leave_one_normal':\n",
        "    for i,data in enumerate(train_tensor):\n",
        "      X,y = data\n",
        "      train_X[i] = X\n",
        "      if y == target_num:\n",
        "        train_y.append(1)\n",
        "      else:\n",
        "        train_y.append(-1)\n",
        "   \n",
        "  train_y = np.array(train_y)\n",
        "\n",
        "  index_contam = np.arange(len(train_y))[train_y == -1]\n",
        "  index_contam = np.random.choice(index_contam,int(contam_ratio*len(index_contam)),replace = False)\n",
        "\n",
        "  train_y[index_contam] = 1\n",
        "\n",
        "  #normal training tensor and dataloader\n",
        "  if not hybrid:\n",
        "    index_subset = np.arange(len(train_y))[train_y == 1]\n",
        "  else:\n",
        "    index_subset = np.arange(len(train_y))\n",
        "\n",
        "  train_tensor = TensorDataset(train_X,torch.tensor(train_y))\n",
        "  train_tensor = Subset(train_tensor,index_subset)\n",
        "  train_loader = torch.utils.data.DataLoader(train_tensor,batch_size = batch_size,shuffle = True,drop_last = True)\n",
        "\n",
        "  ###########################################################################################################\n",
        "  \n",
        "  #testing label\n",
        "  test_X = torch.empty([len(test_tensor),test_tensor[0][0].size(0),img_size,img_size])\n",
        "  test_y = []\n",
        "\n",
        "  for i,data in enumerate(test_tensor):\n",
        "    X,y = data\n",
        "    test_X[i] = X\n",
        "    test_y.append(y)\n",
        "\n",
        "  test_y = np.array(test_y)\n",
        "\n",
        "  if mode == 'leave_one_anomaly':\n",
        "    index_y_normal = np.arange(len(test_y))[test_y != target_num]\n",
        "    index_y_anomaly = np.arange(len(test_y))[test_y == target_num]\n",
        "  elif mode == 'leave_one_normal':\n",
        "    index_y_normal = np.arange(len(test_y))[test_y == target_num]\n",
        "    index_y_anomaly = np.arange(len(test_y))[test_y != target_num]  \n",
        "\n",
        "      \n",
        "  test_y_bin = test_y.copy()\n",
        "  test_y_bin[index_y_normal] = 1\n",
        "  test_y_bin[index_y_anomaly] = -1\n",
        "\n",
        "  return train_loader,test_X,test_y_bin"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nahh0ylbEOxR",
        "trusted": true,
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class generator(nn.Module):\n",
        "  def __init__(self,nc,ngf,nz,n_extra_layers=0):\n",
        "    super(generator,self).__init__()\n",
        "    \n",
        "    self.encoder_1 = nn.Sequential(\n",
        "        #(nc)*32*32\n",
        "        nn.Conv2d(nc, ngf, 4, 2, 1, bias = False),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        #ngf*16*16\n",
        "        nn.Conv2d(ngf, ngf*2, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(ngf*2),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        #(ngf*2)*8*8\n",
        "        nn.Conv2d(ngf*2, ngf*4, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(ngf*4),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        #(ngf*4)*4*4\n",
        "        #final conv layer\n",
        "        nn.Conv2d(ngf*4, nz, 4, 1, 0, bias = False)\n",
        "    )\n",
        "    \n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.ConvTranspose2d(nz, ngf*4, 4, 1, 0, bias = False),\n",
        "        nn.BatchNorm2d(ngf*4),\n",
        "        nn.ReLU(inplace = True),\n",
        "        #(ngf*4)*4*4\n",
        "        nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(ngf*2),\n",
        "        nn.ReLU(inplace = True),\n",
        "        #(ngf*2)*8*8\n",
        "        nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(ngf),\n",
        "        nn.ReLU(inplace = True),\n",
        "        #ngf*16*16\n",
        "        nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias = False),\n",
        "        nn.Tanh()\n",
        "        #nc*32*32\n",
        "    )\n",
        "\n",
        "    self.encoder_2 = nn.Sequential(\n",
        "        #(nc)*32*32\n",
        "        nn.Conv2d(nc, ngf, 4, 2, 1, bias = False),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        #ngf*16*16\n",
        "        nn.Conv2d(ngf, ngf*2, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(ngf*2),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        #(ngf*2)*8*8\n",
        "        nn.Conv2d(ngf*2, ngf*4, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(ngf*4),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        #(ngf*4)*4*4\n",
        "        #final conv layer\n",
        "        nn.Conv2d(ngf*4, nz, 4, 1, 0, bias = False)\n",
        "    )\n",
        "      \n",
        "  def forward(self,X):\n",
        "    z = self.encoder_1(X)\n",
        "    X_hat = self.decoder(z)\n",
        "    z_hat = self.encoder_2(X_hat)\n",
        "\n",
        "    return z,X_hat,z_hat"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrRmmIjcqriD",
        "trusted": true,
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class discriminator(nn.Module):\n",
        "  def __init__(self,nc,ndf,nz):\n",
        "    super(discriminator,self).__init__()\n",
        "    \n",
        "    self.feature = nn.Sequential(\n",
        "        #(nc)*32*32\n",
        "        nn.Conv2d(nc, ndf, 4, 2, 1, bias = False),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        #ndf*16*16\n",
        "        nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(ndf*2),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        #(ndf*2)*8*8\n",
        "        nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias = False),\n",
        "        nn.BatchNorm2d(ndf*4),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        #(ndf*4)*4*4\n",
        "        )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        #final conv layer\n",
        "        nn.Conv2d(ndf*4, 1, 4, 1, 0, bias = False),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "\n",
        "    \n",
        "  def forward(self,X):\n",
        "    feature=self.feature(X)\n",
        "    output=self.classifier(feature).view(-1,1).squeeze(1)\n",
        "\n",
        "    return feature,output"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZuVSxQZScd-",
        "trusted": true,
        "scrolled": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(mod):\n",
        "  \"\"\"\n",
        "  Custom weights initialization called on netG, netD and netE\n",
        "  :param m:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  classname = mod.__class__.__name__\n",
        "  if classname.find('Conv') != -1:\n",
        "    mod.weight.data.normal_(0.0, 0.02)\n",
        "  elif classname.find('BatchNorm') != -1:\n",
        "    mod.weight.data.normal_(1.0, 0.02)\n",
        "    mod.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "# net_generator.apply(weights_init)\n",
        "# net_discriminator.apply(weights_init)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX8hIWGkx8I3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(dataloader,net_generator,net_discriminator,eta,epochs,batch_size,print_loss):\n",
        "  L1_criterion = nn.L1Loss(reduction='none')\n",
        "  L2_criterion = nn.MSELoss(reduction='none')\n",
        "  BCE_criterion = nn.BCELoss(reduction='mean')\n",
        "\n",
        "  if is_cuda:\n",
        "    L1_criterion.cuda()\n",
        "    L2_criterion.cuda()\n",
        "    BCE_criterion.cuda()\n",
        "\n",
        "  loss_D_list = []\n",
        "  loss_G_list = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for i,data in enumerate(dataloader):\n",
        "      \n",
        "      X,y_aclabel = data\n",
        "      y_real = torch.FloatTensor(batch_size).fill_(0)#real label=0,size=batch_size\n",
        "      y_fake = torch.FloatTensor(batch_size).fill_(1)#fake label=1,size=batch_size\n",
        "\n",
        "      if is_cuda:\n",
        "        X = X.cuda()\n",
        "        y_aclabel = y_aclabel.cuda()\n",
        "        y_real = y_real.cuda()\n",
        "        y_fake = y_fake.cuda()\n",
        "\n",
        "      X = Variable(X)\n",
        "      y_aclabel = Variable(y_aclabel)\n",
        "      y_real = Variable(y_real)\n",
        "      y_fake = Variable(y_fake)\n",
        "\n",
        "      #zero grad for discriminator\n",
        "      net_discriminator.zero_grad()\n",
        "      #training the discriminator with real sample\n",
        "      _,output = net_discriminator(X)\n",
        "      loss_D_real = BCE_criterion(output,y_real)\n",
        "\n",
        "      #training the discriminator with fake sample\n",
        "      _,X_hat,_ = net_generator(X)\n",
        "      _,output = net_discriminator(X_hat)\n",
        "\n",
        "      loss_D_fake = BCE_criterion(output,y_fake)\n",
        "\n",
        "      #entire loss in discriminator\n",
        "      loss_D = (loss_D_real+loss_D_fake)/2\n",
        "      \n",
        "      loss_D.backward()\n",
        "      optimizer_D.step()\n",
        "      \n",
        "      if loss_D<1e-5:\n",
        "        net_discriminator.apply(weights_init)\n",
        "\n",
        "      #training the generator based on the result from the discriminator\n",
        "      net_generator.zero_grad()\n",
        "\n",
        "      z,X_hat,z_hat = net_generator(X)\n",
        "\n",
        "      #latent loss\n",
        "      feature_real,_ = net_discriminator(X)\n",
        "      feature_fake,_ = net_discriminator(X_hat)\n",
        "\n",
        "      loss_G_latent = torch.mean(L2_criterion(feature_fake,feature_real).view(batch_size,-1),1)\n",
        "      \n",
        "      #contexutal loss\n",
        "      loss_G_contextual = torch.mean(L1_criterion(X,X_hat).view(batch_size,-1),1)\n",
        "      #entire loss in generator\n",
        "\n",
        "      #encoder loss\n",
        "      loss_G_encoder = torch.mean(L2_criterion(z,z_hat).view(batch_size,-1),1)\n",
        "      \n",
        "      loss_G = 1*loss_G_latent + 50*loss_G_contextual + 1*loss_G_encoder\n",
        "      \n",
        "      loss_G_normal = (loss_G[y_aclabel == 1])\n",
        "      loss_G_anomaly = pow(loss_G[y_aclabel == -1],-1)\n",
        "      \n",
        "      if loss_G_anomaly.size(0)>0:\n",
        "        loss_G = (1-eta)*(sum(loss_G_normal)/loss_G_normal.size(0))+\\\n",
        "                eta*(sum(loss_G_anomaly)/loss_G_anomaly.size(0))\n",
        "      else:\n",
        "        loss_G = sum(loss_G_normal)/loss_G_normal.size(0)\n",
        "      \n",
        "      loss_G.backward()\n",
        "      optimizer_G.step()\n",
        "\n",
        "      if i%50 == 0:\n",
        "        print('[%d/%d] [%d/%d] Loss D: %.4f / Loss G: %.4f' % (epoch+1,epochs,i,len(dataloader),loss_D,loss_G))\n",
        "\n",
        "      loss_D_list.append(loss_D)\n",
        "      loss_G_list.append(loss_G)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTvL3cb8yxl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation(test_X,model):\n",
        "  z,X_hat,z_hat = model(test_X.cuda())\n",
        "  #calculate the anomaly score in testing sample\n",
        "  L2_criterion = nn.MSELoss(reduction='none')\n",
        "  score = L2_criterion(z.cpu(),z_hat.cpu())\n",
        "\n",
        "  #sum the score in each sample\n",
        "  score = torch.sum(score,dim=1)\n",
        "  score = score.view(-1).detach().numpy()\n",
        "\n",
        "  return score"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7nfO85vBv0sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = 'MNIST'\n",
        "mode = 'leave_one_anomaly'\n",
        "\n",
        "#anomaly number (leave one class out)\n",
        "target_nums = [0,1,2,3,4,5,6,7,8,9]\n",
        "\n",
        "#seed\n",
        "seeds = [1,2,3,4,5]\n",
        "\n",
        "#contaminating ratio\n",
        "contam_ratios = [1.0,0.75,0.5,0.25,0.0]\n",
        "\n",
        "#hyper-parameter of eta\n",
        "eta = 0.9"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6ObPrdbFNQL",
        "trusted": true,
        "scrolled": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_size = 32\n",
        "batch_size = 512 #default = 64\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "epochs= 15 #default = 15\n",
        "\n",
        "#size of latent vector\n",
        "nz = 50 #default = 100\n",
        "\n",
        "#filter size of model\n",
        "ndf = 32 #default = 64\n",
        "ngf = 32 #default = 64\n",
        "\n",
        "#output image channels\n",
        "nc = 1\n",
        "\n",
        "if dataset == 'CIFAR10':\n",
        "  nc = 3"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUELyvviz3sK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_aucpr_mean = pd.DataFrame(data = None, index = target_nums, columns = contam_ratios)\n",
        "df_aucpr_std = pd.DataFrame(data = None, index = target_nums, columns = contam_ratios)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOl9RC930Fkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f56524a0-468f-4e04-ae03-5c76c034ed80"
      },
      "source": [
        "for ratio in contam_ratios:\n",
        "\n",
        "  df_aucpr_local = pd.DataFrame(data = None, index = target_nums, columns = seeds)\n",
        "\n",
        "  for num in target_nums:\n",
        "    for seed in seeds:\n",
        "      #data\n",
        "      train_loader,test_X,test_y_bin = data_prepare(dataset = dataset, batch_size = batch_size, target_num = num, contam_ratio = ratio, seed = seed, hybrid = True)\n",
        "\n",
        "      #model initializaiton\n",
        "      set_seed(seed)\n",
        "      net_generator = generator(nc = nc, ngf= ngf, nz = nz)\n",
        "      net_discriminator = discriminator(nc = nc, ndf= ndf, nz = nz)\n",
        "\n",
        "      if is_cuda:\n",
        "        net_generator.cuda()\n",
        "        net_discriminator.cuda()\n",
        "\n",
        "      #weights initialization\n",
        "      net_generator.apply(weights_init)\n",
        "      net_discriminator.apply(weights_init)\n",
        "\n",
        "      optimizer_G = torch.optim.Adam(net_generator.parameters(), lr, betas=(beta1, 0.999))\n",
        "      optimizer_D = torch.optim.Adam(net_discriminator.parameters(), lr, betas=(beta1, 0.999))\n",
        "\n",
        "      fit(dataloader = train_loader, \n",
        "        net_generator = net_generator, net_discriminator = net_discriminator, eta = eta,\n",
        "        epochs = epochs, batch_size = batch_size, print_loss = True)\n",
        "      \n",
        "      #testing\n",
        "      score = evaluation(test_X,net_generator)\n",
        "\n",
        "      df_aucpr_local.loc[num,seed] = average_precision_score(y_true = test_y_bin, y_score = score, pos_label = -1)\n",
        "\n",
        "  #store the result\n",
        "  df_aucpr_mean.loc[:,ratio] = df_aucpr_local.apply(np.mean,axis=1)\n",
        "  df_aucpr_std.loc[:,ratio] = df_aucpr_local.apply(np.std,axis=1)\n",
        "\n",
        "df_aucpr_mean.to_csv('gan_aeplus_hybrid_mean_' + dataset + '.csv',index = True)\n",
        "df_aucpr_std.to_csv('gan_aeplus_hybrid_std_' + dataset + '.csv',index = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/15] [0/117] Loss D: 0.6602 / Loss G: 33.8607\n",
            "[1/15] [50/117] Loss D: 0.0046 / Loss G: 19.4437\n",
            "[1/15] [100/117] Loss D: 0.0026 / Loss G: 16.0507\n",
            "[2/15] [0/117] Loss D: 0.0024 / Loss G: 15.8818\n",
            "[2/15] [50/117] Loss D: 0.0008 / Loss G: 14.5028\n",
            "[2/15] [100/117] Loss D: 0.0006 / Loss G: 13.8074\n",
            "[3/15] [0/117] Loss D: 0.0006 / Loss G: 13.7995\n",
            "[3/15] [50/117] Loss D: 0.0004 / Loss G: 13.2305\n",
            "[3/15] [100/117] Loss D: 0.0002 / Loss G: 12.7476\n",
            "[4/15] [0/117] Loss D: 0.0002 / Loss G: 12.9460\n",
            "[4/15] [50/117] Loss D: 0.0002 / Loss G: 12.7777\n",
            "[4/15] [100/117] Loss D: 0.0002 / Loss G: 12.1628\n",
            "[5/15] [0/117] Loss D: 0.0001 / Loss G: 12.2871\n",
            "[5/15] [50/117] Loss D: 0.0001 / Loss G: 12.0702\n",
            "[5/15] [100/117] Loss D: 0.0001 / Loss G: 11.8295\n",
            "[6/15] [0/117] Loss D: 0.0001 / Loss G: 11.8495\n",
            "[6/15] [50/117] Loss D: 0.0001 / Loss G: 11.7422\n",
            "[6/15] [100/117] Loss D: 0.0001 / Loss G: 11.8258\n",
            "[7/15] [0/117] Loss D: 0.0001 / Loss G: 11.8857\n",
            "[7/15] [50/117] Loss D: 0.0001 / Loss G: 11.7097\n",
            "[7/15] [100/117] Loss D: 0.0001 / Loss G: 11.6496\n",
            "[8/15] [0/117] Loss D: 0.0000 / Loss G: 11.5699\n",
            "[8/15] [50/117] Loss D: 0.0000 / Loss G: 11.3316\n",
            "[8/15] [100/117] Loss D: 0.0000 / Loss G: 11.3955\n",
            "[9/15] [0/117] Loss D: 0.0000 / Loss G: 11.2717\n",
            "[9/15] [50/117] Loss D: 0.0000 / Loss G: 11.4291\n",
            "[9/15] [100/117] Loss D: 0.0000 / Loss G: 11.3136\n",
            "[10/15] [0/117] Loss D: 0.0000 / Loss G: 11.2991\n",
            "[10/15] [50/117] Loss D: 0.0000 / Loss G: 11.3974\n",
            "[10/15] [100/117] Loss D: 0.0000 / Loss G: 11.2721\n",
            "[11/15] [0/117] Loss D: 0.0000 / Loss G: 11.2450\n",
            "[11/15] [50/117] Loss D: 0.0000 / Loss G: 11.0858\n",
            "[11/15] [100/117] Loss D: 0.0000 / Loss G: 11.2501\n",
            "[12/15] [0/117] Loss D: 0.0000 / Loss G: 11.1664\n",
            "[12/15] [50/117] Loss D: 0.0000 / Loss G: 11.2619\n",
            "[12/15] [100/117] Loss D: 0.0000 / Loss G: 11.0633\n",
            "[13/15] [0/117] Loss D: 0.0000 / Loss G: 11.0132\n",
            "[13/15] [50/117] Loss D: 0.0000 / Loss G: 11.0611\n",
            "[13/15] [100/117] Loss D: 0.0000 / Loss G: 11.0436\n",
            "[14/15] [0/117] Loss D: 0.0000 / Loss G: 10.7399\n",
            "[14/15] [50/117] Loss D: 0.0000 / Loss G: 10.9134\n",
            "[14/15] [100/117] Loss D: 0.0000 / Loss G: 10.9897\n",
            "[15/15] [0/117] Loss D: 0.0000 / Loss G: 10.8995\n",
            "[15/15] [50/117] Loss D: 0.0101 / Loss G: 10.3538\n",
            "[15/15] [100/117] Loss D: 0.0019 / Loss G: 10.3907\n",
            "[1/15] [0/117] Loss D: 0.7527 / Loss G: 35.4999\n",
            "[1/15] [50/117] Loss D: 0.0070 / Loss G: 19.1034\n",
            "[1/15] [100/117] Loss D: 0.0039 / Loss G: 16.0492\n",
            "[2/15] [0/117] Loss D: 0.0022 / Loss G: 15.7563\n",
            "[2/15] [50/117] Loss D: 0.0017 / Loss G: 14.5907\n",
            "[2/15] [100/117] Loss D: 0.0011 / Loss G: 13.6808\n",
            "[3/15] [0/117] Loss D: 0.0009 / Loss G: 13.6480\n",
            "[3/15] [50/117] Loss D: 0.0007 / Loss G: 12.8580\n",
            "[3/15] [100/117] Loss D: 0.0006 / Loss G: 12.3633\n",
            "[4/15] [0/117] Loss D: 0.0003 / Loss G: 12.5982\n",
            "[4/15] [50/117] Loss D: 0.0003 / Loss G: 12.2899\n",
            "[4/15] [100/117] Loss D: 0.0002 / Loss G: 12.2098\n",
            "[5/15] [0/117] Loss D: 0.0002 / Loss G: 12.2649\n",
            "[5/15] [50/117] Loss D: 0.0002 / Loss G: 12.0592\n",
            "[5/15] [100/117] Loss D: 0.0002 / Loss G: 11.8332\n",
            "[6/15] [0/117] Loss D: 0.0002 / Loss G: 11.9260\n",
            "[6/15] [50/117] Loss D: 0.0001 / Loss G: 11.9975\n",
            "[6/15] [100/117] Loss D: 0.0001 / Loss G: 11.6072\n",
            "[7/15] [0/117] Loss D: 0.0001 / Loss G: 11.6441\n",
            "[7/15] [50/117] Loss D: 0.0001 / Loss G: 11.5692\n",
            "[7/15] [100/117] Loss D: 0.0001 / Loss G: 11.8398\n",
            "[8/15] [0/117] Loss D: 0.0001 / Loss G: 11.6272\n",
            "[8/15] [50/117] Loss D: 0.0001 / Loss G: 11.6931\n",
            "[8/15] [100/117] Loss D: 0.0000 / Loss G: 11.6440\n",
            "[9/15] [0/117] Loss D: 0.0001 / Loss G: 11.7967\n",
            "[9/15] [50/117] Loss D: 0.0001 / Loss G: 11.0659\n",
            "[9/15] [100/117] Loss D: 0.0001 / Loss G: 11.5338\n",
            "[10/15] [0/117] Loss D: 0.0000 / Loss G: 11.2261\n",
            "[10/15] [50/117] Loss D: 0.0000 / Loss G: 11.2118\n",
            "[10/15] [100/117] Loss D: 0.0000 / Loss G: 11.3857\n",
            "[11/15] [0/117] Loss D: 0.0000 / Loss G: 11.0241\n",
            "[11/15] [50/117] Loss D: 0.0000 / Loss G: 11.0258\n",
            "[11/15] [100/117] Loss D: 0.0000 / Loss G: 11.1305\n",
            "[12/15] [0/117] Loss D: 0.0000 / Loss G: 11.0271\n",
            "[12/15] [50/117] Loss D: 0.0000 / Loss G: 11.2278\n",
            "[12/15] [100/117] Loss D: 0.0001 / Loss G: 11.0728\n",
            "[13/15] [0/117] Loss D: 0.0000 / Loss G: 11.2202\n",
            "[13/15] [50/117] Loss D: 0.0000 / Loss G: 11.4804\n",
            "[13/15] [100/117] Loss D: 0.0000 / Loss G: 10.8675\n",
            "[14/15] [0/117] Loss D: 0.0000 / Loss G: 11.0907\n",
            "[14/15] [50/117] Loss D: 0.0000 / Loss G: 11.0010\n",
            "[14/15] [100/117] Loss D: 0.0000 / Loss G: 10.9908\n",
            "[15/15] [0/117] Loss D: 0.0000 / Loss G: 10.6160\n",
            "[15/15] [50/117] Loss D: 0.0000 / Loss G: 10.7935\n",
            "[15/15] [100/117] Loss D: 0.0000 / Loss G: 10.7408\n",
            "[1/15] [0/117] Loss D: 0.8808 / Loss G: 35.9596\n",
            "[1/15] [50/117] Loss D: 0.0056 / Loss G: 19.2655\n",
            "[1/15] [100/117] Loss D: 0.0030 / Loss G: 15.7911\n",
            "[2/15] [0/117] Loss D: 0.0029 / Loss G: 15.2956\n",
            "[2/15] [50/117] Loss D: 0.0012 / Loss G: 13.8147\n",
            "[2/15] [100/117] Loss D: 0.0007 / Loss G: 13.0965\n",
            "[3/15] [0/117] Loss D: 0.0006 / Loss G: 13.6821\n",
            "[3/15] [50/117] Loss D: 0.0006 / Loss G: 12.7736\n",
            "[3/15] [100/117] Loss D: 0.0004 / Loss G: 12.8258\n",
            "[4/15] [0/117] Loss D: 0.0004 / Loss G: 12.8454\n",
            "[4/15] [50/117] Loss D: 0.0003 / Loss G: 12.4264\n",
            "[4/15] [100/117] Loss D: 0.0002 / Loss G: 12.1380\n",
            "[5/15] [0/117] Loss D: 0.0002 / Loss G: 12.2074\n",
            "[5/15] [50/117] Loss D: 0.0002 / Loss G: 12.2773\n",
            "[5/15] [100/117] Loss D: 0.0001 / Loss G: 11.9296\n",
            "[6/15] [0/117] Loss D: 0.0001 / Loss G: 11.6307\n",
            "[6/15] [50/117] Loss D: 0.0001 / Loss G: 11.9603\n",
            "[6/15] [100/117] Loss D: 0.0001 / Loss G: 11.4709\n",
            "[7/15] [0/117] Loss D: 0.0001 / Loss G: 11.7748\n",
            "[7/15] [50/117] Loss D: 0.0001 / Loss G: 11.5770\n",
            "[7/15] [100/117] Loss D: 0.0001 / Loss G: 11.2807\n",
            "[8/15] [0/117] Loss D: 0.0001 / Loss G: 11.1334\n",
            "[8/15] [50/117] Loss D: 0.0001 / Loss G: 11.8009\n",
            "[8/15] [100/117] Loss D: 0.0001 / Loss G: 11.1351\n",
            "[9/15] [0/117] Loss D: 0.0001 / Loss G: 11.0246\n",
            "[9/15] [50/117] Loss D: 0.0001 / Loss G: 10.9735\n",
            "[9/15] [100/117] Loss D: 0.0000 / Loss G: 10.9481\n",
            "[10/15] [0/117] Loss D: 0.0001 / Loss G: 11.2735\n",
            "[10/15] [50/117] Loss D: 0.0000 / Loss G: 11.2764\n",
            "[10/15] [100/117] Loss D: 0.0000 / Loss G: 11.0603\n",
            "[11/15] [0/117] Loss D: 0.0000 / Loss G: 11.1613\n",
            "[11/15] [50/117] Loss D: 0.0000 / Loss G: 11.4130\n",
            "[11/15] [100/117] Loss D: 0.0000 / Loss G: 11.2647\n",
            "[12/15] [0/117] Loss D: 0.0000 / Loss G: 11.2851\n",
            "[12/15] [50/117] Loss D: 0.0000 / Loss G: 11.0835\n",
            "[12/15] [100/117] Loss D: 0.0000 / Loss G: 11.0325\n",
            "[13/15] [0/117] Loss D: 0.0000 / Loss G: 11.1214\n",
            "[13/15] [50/117] Loss D: 0.0000 / Loss G: 11.1777\n",
            "[13/15] [100/117] Loss D: 0.0000 / Loss G: 11.1616\n",
            "[14/15] [0/117] Loss D: 0.0000 / Loss G: 11.1464\n",
            "[14/15] [50/117] Loss D: 0.0000 / Loss G: 10.6757\n",
            "[14/15] [100/117] Loss D: 0.0000 / Loss G: 10.9536\n",
            "[15/15] [0/117] Loss D: 0.0000 / Loss G: 11.1104\n",
            "[15/15] [50/117] Loss D: 0.0000 / Loss G: 10.9044\n",
            "[15/15] [100/117] Loss D: 0.0000 / Loss G: 10.9501\n",
            "[1/15] [0/117] Loss D: 0.6804 / Loss G: 34.3162\n",
            "[1/15] [50/117] Loss D: 0.0068 / Loss G: 18.4899\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}