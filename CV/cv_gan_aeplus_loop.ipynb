{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvUwidZYEFuc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io\n",
    "\n",
    "#split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#accuracy,AUCROC,precision,recall,f1-score,AUCPR\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#gridsearch/randomsearch\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "#visualize results\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cQ6WYUvgEOsX",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Subset,DataLoader,TensorDataset\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JuUsEkemUC0v"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "46w-n5jpG0wY",
    "outputId": "52f63ebc-1c70-47de-fa67-ecee08a11d4f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is on\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  is_cuda=True\n",
    "  print('GPU is on')\n",
    "else:\n",
    "  is_cuda=False\n",
    "  print('GPU is off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JH8PPlrwEO16",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def data_prepare(dataset,batch_size,target_num,contam_ratio,seed,hybrid): \n",
    "  #set seed for reproductive results\n",
    "  set_seed(seed)\n",
    "\n",
    "  if dataset == 'MNIST':\n",
    "    transformation = transforms.Compose([transforms.Resize(img_size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    #entire training tensor\n",
    "    train_tensor = torchvision.datasets.MNIST('data/',train=True,transform=transformation,download=True)\n",
    "    #testing tensor\n",
    "    test_tensor = torchvision.datasets.MNIST('data/',train=False,transform=transformation,download=True)\n",
    "\n",
    "  elif dataset == 'FashionMNIST':\n",
    "    transformation = transforms.Compose([transforms.Resize(img_size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5,), (0.5,))])\n",
    "    #entire training tensor\n",
    "    train_tensor = torchvision.datasets.FashionMNIST('data/',train=True,transform=transformation,download=True)\n",
    "    #testing tensor\n",
    "    test_tensor = torchvision.datasets.FashionMNIST('data/',train=False,transform=transformation,download=True)\n",
    "\n",
    "  elif dataset == 'CIFAR10':\n",
    "    transformation = transforms.Compose([transforms.Resize(img_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "    #entire training tensor\n",
    "    train_tensor = torchvision.datasets.CIFAR10('data/',train=True,transform=transformation,download=True)\n",
    "    #testing tensor\n",
    "    test_tensor = torchvision.datasets.CIFAR10('data/',train=False,transform=transformation,download=True)\n",
    "\n",
    "  train_X = torch.empty([len(train_tensor),train_tensor[0][0].size(0),img_size,img_size])\n",
    "  train_y = []\n",
    "\n",
    "  if mode == 'leave_one_anomaly':\n",
    "    #only using normal samples to train the model\n",
    "    for i,data in enumerate(train_tensor):\n",
    "      X,y = data\n",
    "      train_X[i] = X\n",
    "      if y != target_num:\n",
    "        train_y.append(1)\n",
    "      else:\n",
    "        train_y.append(-1)\n",
    "\n",
    "  elif mode == 'leave_one_normal':\n",
    "    for i,data in enumerate(train_tensor):\n",
    "      X,y = data\n",
    "      train_X[i] = X\n",
    "      if y == target_num:\n",
    "        train_y.append(1)\n",
    "      else:\n",
    "        train_y.append(-1)\n",
    "   \n",
    "  train_y = np.array(train_y)\n",
    "\n",
    "  index_contam = np.arange(len(train_y))[train_y == -1]\n",
    "  index_contam = np.random.choice(index_contam,int(contam_ratio*len(index_contam)),replace = False)\n",
    "\n",
    "  train_y[index_contam] = 1\n",
    "\n",
    "  #normal training tensor and dataloader\n",
    "  if not hybrid:\n",
    "    index_subset = np.arange(len(train_y))[train_y == 1]\n",
    "  else:\n",
    "    index_subset = np.arange(len(train_y))\n",
    "\n",
    "  train_tensor = TensorDataset(train_X,torch.tensor(train_y))\n",
    "  train_tensor = Subset(train_tensor,index_subset)\n",
    "  train_loader = torch.utils.data.DataLoader(train_tensor,batch_size = batch_size,shuffle = True,drop_last = True)\n",
    "\n",
    "  ###########################################################################################################\n",
    "  \n",
    "  #testing label\n",
    "  test_X = torch.empty([len(test_tensor),test_tensor[0][0].size(0),img_size,img_size])\n",
    "  test_y = []\n",
    "\n",
    "  for i,data in enumerate(test_tensor):\n",
    "    X,y = data\n",
    "    test_X[i] = X\n",
    "    test_y.append(y)\n",
    "\n",
    "  test_y = np.array(test_y)\n",
    "\n",
    "  if mode == 'leave_one_anomaly':\n",
    "    index_y_normal = np.arange(len(test_y))[test_y != target_num]\n",
    "    index_y_anomaly = np.arange(len(test_y))[test_y == target_num]\n",
    "  elif mode == 'leave_one_normal':\n",
    "    index_y_normal = np.arange(len(test_y))[test_y == target_num]\n",
    "    index_y_anomaly = np.arange(len(test_y))[test_y != target_num]  \n",
    "\n",
    "      \n",
    "  test_y_bin = test_y.copy()\n",
    "  test_y_bin[index_y_normal] = 1\n",
    "  test_y_bin[index_y_anomaly] = -1\n",
    "\n",
    "  return train_loader,test_X,test_y_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nahh0ylbEOxR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "  def __init__(self,nc,ngf,nz,n_extra_layers=0):\n",
    "    super(generator,self).__init__()\n",
    "    \n",
    "    self.encoder_1 = nn.Sequential(\n",
    "        #(nc)*32*32\n",
    "        nn.Conv2d(nc, ngf, 4, 2, 1, bias = False),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        #ngf*16*16\n",
    "        nn.Conv2d(ngf, ngf*2, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(ngf*2),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        #(ngf*2)*8*8\n",
    "        nn.Conv2d(ngf*2, ngf*4, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(ngf*4),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        #(ngf*4)*4*4\n",
    "        #final conv layer\n",
    "        nn.Conv2d(ngf*4, nz, 4, 1, 0, bias = False)\n",
    "    )\n",
    "    \n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.ConvTranspose2d(nz, ngf*4, 4, 1, 0, bias = False),\n",
    "        nn.BatchNorm2d(ngf*4),\n",
    "        nn.ReLU(inplace = True),\n",
    "        #(ngf*4)*4*4\n",
    "        nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(ngf*2),\n",
    "        nn.ReLU(inplace = True),\n",
    "        #(ngf*2)*8*8\n",
    "        nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(ngf),\n",
    "        nn.ReLU(inplace = True),\n",
    "        #ngf*16*16\n",
    "        nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias = False),\n",
    "        nn.Tanh()\n",
    "        #nc*32*32\n",
    "    )\n",
    "\n",
    "    self.encoder_2 = nn.Sequential(\n",
    "        #(nc)*32*32\n",
    "        nn.Conv2d(nc, ngf, 4, 2, 1, bias = False),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        #ngf*16*16\n",
    "        nn.Conv2d(ngf, ngf*2, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(ngf*2),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        #(ngf*2)*8*8\n",
    "        nn.Conv2d(ngf*2, ngf*4, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(ngf*4),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        #(ngf*4)*4*4\n",
    "        #final conv layer\n",
    "        nn.Conv2d(ngf*4, nz, 4, 1, 0, bias = False)\n",
    "    )\n",
    "      \n",
    "  def forward(self,X):\n",
    "    z = self.encoder_1(X)\n",
    "    X_hat = self.decoder(z)\n",
    "    z_hat = self.encoder_2(X_hat)\n",
    "\n",
    "    return z,X_hat,z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrRmmIjcqriD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "  def __init__(self,nc,ndf,nz):\n",
    "    super(discriminator,self).__init__()\n",
    "    \n",
    "    self.feature = nn.Sequential(\n",
    "        #(nc)*32*32\n",
    "        nn.Conv2d(nc, ndf, 4, 2, 1, bias = False),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        #ndf*16*16\n",
    "        nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(ndf*2),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        #(ndf*2)*8*8\n",
    "        nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(ndf*4),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        #(ndf*4)*4*4\n",
    "        )\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "        #final conv layer\n",
    "        nn.Conv2d(ndf*4, 1, 4, 1, 0, bias = False),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "  def forward(self,X):\n",
    "    feature=self.feature(X)\n",
    "    output=self.classifier(feature).view(-1,1).squeeze(1)\n",
    "\n",
    "    return feature,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZuVSxQZScd-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def weights_init(mod):\n",
    "  \"\"\"\n",
    "  Custom weights initialization called on netG, netD and netE\n",
    "  :param m:\n",
    "  :return:\n",
    "  \"\"\"\n",
    "  classname = mod.__class__.__name__\n",
    "  if classname.find('Conv') != -1:\n",
    "      mod.weight.data.normal_(0.0, 0.02)\n",
    "  elif classname.find('BatchNorm') != -1:\n",
    "      mod.weight.data.normal_(1.0, 0.02)\n",
    "      mod.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "# net_generator.apply(weights_init)\n",
    "# net_discriminator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GS2o7Cc2sZBn"
   },
   "outputs": [],
   "source": [
    "def fit(dataloader,net_generator,net_discriminator,epochs,batch_size,print_loss):\n",
    "  L1_criterion = nn.L1Loss(reduction='mean')\n",
    "  L2_criterion = nn.MSELoss(reduction='mean')\n",
    "  BCE_criterion = nn.BCELoss(reduction='mean')\n",
    "\n",
    "  if is_cuda:\n",
    "    L1_criterion.cuda()\n",
    "    L2_criterion.cuda()\n",
    "    BCE_criterion.cuda()\n",
    "\n",
    "  loss_D_list = []\n",
    "  loss_G_list = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    for i,data in enumerate(dataloader):\n",
    "      X,_ = data\n",
    "      y_real = torch.FloatTensor(batch_size).fill_(0)#real label=0,size=batch_size\n",
    "      y_fake = torch.FloatTensor(batch_size).fill_(1)#fake label=1,size=batch_size\n",
    "\n",
    "      if is_cuda:\n",
    "        X = X.cuda()\n",
    "        y_real = y_real.cuda()\n",
    "        y_fake = y_fake.cuda()\n",
    "\n",
    "      X = Variable(X)\n",
    "      y_real = Variable(y_real)\n",
    "      y_fake = Variable(y_fake)\n",
    "\n",
    "      #zero grad for discriminator\n",
    "      net_discriminator.zero_grad()\n",
    "      #training the discriminator with real sample\n",
    "      _,output = net_discriminator(X)\n",
    "      loss_D_real = BCE_criterion(output,y_real)\n",
    "\n",
    "      #training the discriminator with fake sample\n",
    "      _,X_hat,_ = net_generator(X)\n",
    "      _,output = net_discriminator(X_hat)\n",
    "\n",
    "      loss_D_fake = BCE_criterion(output,y_fake)\n",
    "\n",
    "      #entire loss in discriminator\n",
    "      loss_D = (loss_D_real+loss_D_fake)/2\n",
    "      \n",
    "      loss_D.backward()\n",
    "      optimizer_D.step()\n",
    "      \n",
    "      if loss_D<1e-5:\n",
    "          net_discriminator.apply(weights_init)\n",
    "\n",
    "      #training the generator based on the result from the discriminator\n",
    "      net_generator.zero_grad()\n",
    "\n",
    "      z,X_hat,z_hat = net_generator(X)\n",
    "\n",
    "      #adversarial loss\n",
    "      # _,output = net_discriminator(X_hat)\n",
    "      # loss_G_adversarial = BCE_criterion(output,y_real)\n",
    "\n",
    "      #latent loss\n",
    "      feature_real,_ = net_discriminator(X)\n",
    "      feature_fake,_ = net_discriminator(X_hat)\n",
    "\n",
    "      loss_G_latent = L2_criterion(feature_fake,feature_real)\n",
    "      \n",
    "      #contexutal loss\n",
    "      loss_G_contextual = L1_criterion(X,X_hat)\n",
    "      #entire loss in generator\n",
    "\n",
    "      #encoder loss\n",
    "      loss_G_encoder = L2_criterion(z,z_hat)\n",
    "      loss_G = 1*loss_G_latent + 50*loss_G_contextual + 1*loss_G_encoder\n",
    "      #loss_G = 1*loss_G_adversarial + 50*loss_G_contextual + 1*loss_G_encoder\n",
    "\n",
    "      loss_G.backward()\n",
    "      optimizer_G.step()\n",
    "\n",
    "      if i%50 == 0 & print_loss:\n",
    "        print('[%d/%d] [%d/%d] Loss D: %.4f / Loss G: %.4f' % (epoch+1,epochs,i,len(dataloader),loss_D,loss_G))\n",
    "\n",
    "      loss_D_list.append(loss_D)\n",
    "      loss_G_list.append(loss_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wIhrAtOwu8aB"
   },
   "outputs": [],
   "source": [
    "def evaluation(test_X,model):\n",
    "  z,X_hat,z_hat = model(test_X.cuda())\n",
    "  #calculate the anomaly score in testing sample\n",
    "  L2_criterion = nn.MSELoss(reduction='none')\n",
    "  score = L2_criterion(z.cpu(),z_hat.cpu())\n",
    "\n",
    "  #sum the score in each sample\n",
    "  score = torch.sum(score,dim=1)\n",
    "  score = score.view(-1).detach().numpy()\n",
    "\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kvEPlluV4AO1"
   },
   "outputs": [],
   "source": [
    "dataset = 'MNIST'\n",
    "mode = 'leave_one_anomaly'\n",
    "\n",
    "#anomaly number (leave one class out)\n",
    "target_nums = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "#seed\n",
    "seeds = [1,2,3,4,5]\n",
    "\n",
    "#contaminating ratio\n",
    "contam_ratios = [1.0,0.75,0.5,0.25,0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iW4Ca74ctLvM"
   },
   "outputs": [],
   "source": [
    "#global parameters\n",
    "img_size = 32\n",
    "batch_size = 512 #default = 64\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "epochs= 15 #default = 15\n",
    "\n",
    "#size of latent vector\n",
    "nz = 50 #default = 100\n",
    "\n",
    "#filter size of generator\n",
    "ngf = 32 #default = 64\n",
    "#filter size of discriminator\n",
    "ndf = 32 #default = 64\n",
    "\n",
    "#output image channels\n",
    "nc = 1\n",
    "\n",
    "if dataset == 'CIFAR10':\n",
    "  nc = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hqh23DIzvdpZ"
   },
   "outputs": [],
   "source": [
    "df_aucpr_mean = pd.DataFrame(data = None, index = target_nums, columns = contam_ratios)\n",
    "df_aucpr_std = pd.DataFrame(data = None, index = target_nums, columns = contam_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xkZ94-HT3es5",
    "outputId": "8068dd74-1ec3-4759-80cb-154025229b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/15] [0/117] Loss D: 0.6602 / Loss G: 33.8607\n",
      "[1/15] [50/117] Loss D: 0.0046 / Loss G: 19.4437\n",
      "[1/15] [100/117] Loss D: 0.0026 / Loss G: 16.0507\n",
      "[2/15] [0/117] Loss D: 0.0024 / Loss G: 15.8818\n",
      "[2/15] [50/117] Loss D: 0.0008 / Loss G: 14.5028\n",
      "[2/15] [100/117] Loss D: 0.0006 / Loss G: 13.8074\n",
      "[3/15] [0/117] Loss D: 0.0006 / Loss G: 13.7995\n",
      "[3/15] [50/117] Loss D: 0.0004 / Loss G: 13.2305\n",
      "[3/15] [100/117] Loss D: 0.0002 / Loss G: 12.7476\n",
      "[4/15] [0/117] Loss D: 0.0002 / Loss G: 12.9460\n",
      "[4/15] [50/117] Loss D: 0.0002 / Loss G: 12.7777\n",
      "[4/15] [100/117] Loss D: 0.0002 / Loss G: 12.1628\n",
      "[5/15] [0/117] Loss D: 0.0001 / Loss G: 12.2871\n",
      "[5/15] [50/117] Loss D: 0.0001 / Loss G: 12.0702\n",
      "[5/15] [100/117] Loss D: 0.0001 / Loss G: 11.8295\n",
      "[6/15] [0/117] Loss D: 0.0001 / Loss G: 11.8495\n",
      "[6/15] [50/117] Loss D: 0.0001 / Loss G: 11.7422\n",
      "[6/15] [100/117] Loss D: 0.0001 / Loss G: 11.8258\n",
      "[7/15] [0/117] Loss D: 0.0001 / Loss G: 11.8857\n",
      "[7/15] [50/117] Loss D: 0.0001 / Loss G: 11.7097\n",
      "[7/15] [100/117] Loss D: 0.0001 / Loss G: 11.6496\n",
      "[8/15] [0/117] Loss D: 0.0000 / Loss G: 11.5699\n",
      "[8/15] [50/117] Loss D: 0.0000 / Loss G: 11.3316\n",
      "[8/15] [100/117] Loss D: 0.0000 / Loss G: 11.3955\n",
      "[9/15] [0/117] Loss D: 0.0000 / Loss G: 11.2717\n",
      "[9/15] [50/117] Loss D: 0.0000 / Loss G: 11.4291\n",
      "[9/15] [100/117] Loss D: 0.0000 / Loss G: 11.3136\n",
      "[10/15] [0/117] Loss D: 0.0000 / Loss G: 11.2991\n",
      "[10/15] [50/117] Loss D: 0.0000 / Loss G: 11.3974\n",
      "[10/15] [100/117] Loss D: 0.0000 / Loss G: 11.2721\n",
      "[11/15] [0/117] Loss D: 0.0000 / Loss G: 11.2450\n",
      "[11/15] [50/117] Loss D: 0.0000 / Loss G: 11.0858\n",
      "[11/15] [100/117] Loss D: 0.0000 / Loss G: 11.2501\n",
      "[12/15] [0/117] Loss D: 0.0000 / Loss G: 11.1664\n",
      "[12/15] [50/117] Loss D: 0.0000 / Loss G: 11.2619\n",
      "[12/15] [100/117] Loss D: 0.0000 / Loss G: 11.0633\n",
      "[13/15] [0/117] Loss D: 0.0000 / Loss G: 11.0132\n",
      "[13/15] [50/117] Loss D: 0.0000 / Loss G: 11.0611\n",
      "[13/15] [100/117] Loss D: 0.0000 / Loss G: 11.0436\n",
      "[14/15] [0/117] Loss D: 0.0000 / Loss G: 10.7399\n",
      "[14/15] [50/117] Loss D: 0.0000 / Loss G: 10.9134\n",
      "[14/15] [100/117] Loss D: 0.0000 / Loss G: 10.9897\n",
      "[15/15] [0/117] Loss D: 0.0000 / Loss G: 10.8995\n",
      "[15/15] [50/117] Loss D: 0.0101 / Loss G: 10.3538\n",
      "[15/15] [100/117] Loss D: 0.0019 / Loss G: 10.3907\n",
      "[1/15] [0/117] Loss D: 0.7527 / Loss G: 35.4999\n",
      "[1/15] [50/117] Loss D: 0.0070 / Loss G: 19.1034\n",
      "[1/15] [100/117] Loss D: 0.0039 / Loss G: 16.0492\n",
      "[2/15] [0/117] Loss D: 0.0022 / Loss G: 15.7563\n",
      "[2/15] [50/117] Loss D: 0.0017 / Loss G: 14.5907\n",
      "[2/15] [100/117] Loss D: 0.0011 / Loss G: 13.6808\n",
      "[3/15] [0/117] Loss D: 0.0009 / Loss G: 13.6480\n",
      "[3/15] [50/117] Loss D: 0.0007 / Loss G: 12.8580\n",
      "[3/15] [100/117] Loss D: 0.0006 / Loss G: 12.3633\n",
      "[4/15] [0/117] Loss D: 0.0003 / Loss G: 12.5982\n",
      "[4/15] [50/117] Loss D: 0.0003 / Loss G: 12.2899\n",
      "[4/15] [100/117] Loss D: 0.0002 / Loss G: 12.2098\n",
      "[5/15] [0/117] Loss D: 0.0002 / Loss G: 12.2649\n",
      "[5/15] [50/117] Loss D: 0.0002 / Loss G: 12.0592\n",
      "[5/15] [100/117] Loss D: 0.0002 / Loss G: 11.8332\n",
      "[6/15] [0/117] Loss D: 0.0002 / Loss G: 11.9260\n",
      "[6/15] [50/117] Loss D: 0.0001 / Loss G: 11.9975\n",
      "[6/15] [100/117] Loss D: 0.0001 / Loss G: 11.6072\n",
      "[7/15] [0/117] Loss D: 0.0001 / Loss G: 11.6441\n",
      "[7/15] [50/117] Loss D: 0.0001 / Loss G: 11.5692\n",
      "[7/15] [100/117] Loss D: 0.0001 / Loss G: 11.8398\n",
      "[8/15] [0/117] Loss D: 0.0001 / Loss G: 11.6272\n",
      "[8/15] [50/117] Loss D: 0.0001 / Loss G: 11.6931\n",
      "[8/15] [100/117] Loss D: 0.0000 / Loss G: 11.6440\n",
      "[9/15] [0/117] Loss D: 0.0001 / Loss G: 11.7967\n",
      "[9/15] [50/117] Loss D: 0.0001 / Loss G: 11.0659\n",
      "[9/15] [100/117] Loss D: 0.0001 / Loss G: 11.5338\n",
      "[10/15] [0/117] Loss D: 0.0000 / Loss G: 11.2261\n",
      "[10/15] [50/117] Loss D: 0.0000 / Loss G: 11.2118\n",
      "[10/15] [100/117] Loss D: 0.0000 / Loss G: 11.3857\n",
      "[11/15] [0/117] Loss D: 0.0000 / Loss G: 11.0241\n",
      "[11/15] [50/117] Loss D: 0.0000 / Loss G: 11.0258\n",
      "[11/15] [100/117] Loss D: 0.0000 / Loss G: 11.1305\n",
      "[12/15] [0/117] Loss D: 0.0000 / Loss G: 11.0271\n",
      "[12/15] [50/117] Loss D: 0.0000 / Loss G: 11.2278\n",
      "[12/15] [100/117] Loss D: 0.0001 / Loss G: 11.0729\n",
      "[13/15] [0/117] Loss D: 0.0000 / Loss G: 11.2202\n",
      "[13/15] [50/117] Loss D: 0.0000 / Loss G: 11.4804\n",
      "[13/15] [100/117] Loss D: 0.0000 / Loss G: 10.8675\n",
      "[14/15] [0/117] Loss D: 0.0000 / Loss G: 11.0907\n",
      "[14/15] [50/117] Loss D: 0.0000 / Loss G: 11.0010\n",
      "[14/15] [100/117] Loss D: 0.0000 / Loss G: 10.9908\n",
      "[15/15] [0/117] Loss D: 0.0000 / Loss G: 10.6160\n",
      "[15/15] [50/117] Loss D: 0.0000 / Loss G: 10.7935\n",
      "[15/15] [100/117] Loss D: 0.0000 / Loss G: 10.7408\n",
      "[1/15] [0/117] Loss D: 0.8808 / Loss G: 35.9596\n",
      "[1/15] [50/117] Loss D: 0.0056 / Loss G: 19.2655\n",
      "[1/15] [100/117] Loss D: 0.0030 / Loss G: 15.7911\n",
      "[2/15] [0/117] Loss D: 0.0029 / Loss G: 15.2956\n",
      "[2/15] [50/117] Loss D: 0.0012 / Loss G: 13.8147\n",
      "[2/15] [100/117] Loss D: 0.0007 / Loss G: 13.0965\n",
      "[3/15] [0/117] Loss D: 0.0006 / Loss G: 13.6820\n",
      "[3/15] [50/117] Loss D: 0.0006 / Loss G: 12.7736\n",
      "[3/15] [100/117] Loss D: 0.0004 / Loss G: 12.8257\n",
      "[4/15] [0/117] Loss D: 0.0004 / Loss G: 12.8454\n",
      "[4/15] [50/117] Loss D: 0.0003 / Loss G: 12.4264\n",
      "[4/15] [100/117] Loss D: 0.0002 / Loss G: 12.1380\n",
      "[5/15] [0/117] Loss D: 0.0002 / Loss G: 12.2074\n",
      "[5/15] [50/117] Loss D: 0.0002 / Loss G: 12.2773\n",
      "[5/15] [100/117] Loss D: 0.0001 / Loss G: 11.9296\n",
      "[6/15] [0/117] Loss D: 0.0001 / Loss G: 11.6307\n",
      "[6/15] [50/117] Loss D: 0.0001 / Loss G: 11.9603\n",
      "[6/15] [100/117] Loss D: 0.0001 / Loss G: 11.4709\n",
      "[7/15] [0/117] Loss D: 0.0001 / Loss G: 11.7748\n",
      "[7/15] [50/117] Loss D: 0.0001 / Loss G: 11.5770\n",
      "[7/15] [100/117] Loss D: 0.0001 / Loss G: 11.2807\n",
      "[8/15] [0/117] Loss D: 0.0001 / Loss G: 11.1334\n",
      "[8/15] [50/117] Loss D: 0.0001 / Loss G: 11.8009\n",
      "[8/15] [100/117] Loss D: 0.0001 / Loss G: 11.1351\n",
      "[9/15] [0/117] Loss D: 0.0001 / Loss G: 11.0246\n",
      "[9/15] [50/117] Loss D: 0.0001 / Loss G: 10.9735\n",
      "[9/15] [100/117] Loss D: 0.0000 / Loss G: 10.9481\n",
      "[10/15] [0/117] Loss D: 0.0001 / Loss G: 11.2735\n",
      "[10/15] [50/117] Loss D: 0.0000 / Loss G: 11.2764\n",
      "[10/15] [100/117] Loss D: 0.0000 / Loss G: 11.0603\n",
      "[11/15] [0/117] Loss D: 0.0000 / Loss G: 11.1613\n",
      "[11/15] [50/117] Loss D: 0.0000 / Loss G: 11.4130\n",
      "[11/15] [100/117] Loss D: 0.0000 / Loss G: 11.2647\n",
      "[12/15] [0/117] Loss D: 0.0000 / Loss G: 11.2851\n",
      "[12/15] [50/117] Loss D: 0.0000 / Loss G: 11.0835\n",
      "[12/15] [100/117] Loss D: 0.0000 / Loss G: 11.0325\n",
      "[13/15] [0/117] Loss D: 0.0000 / Loss G: 11.1214\n",
      "[13/15] [50/117] Loss D: 0.0000 / Loss G: 11.1777\n",
      "[13/15] [100/117] Loss D: 0.0000 / Loss G: 11.1616\n",
      "[14/15] [0/117] Loss D: 0.0000 / Loss G: 11.1464\n",
      "[14/15] [50/117] Loss D: 0.0000 / Loss G: 10.6757\n",
      "[14/15] [100/117] Loss D: 0.0000 / Loss G: 10.9536\n",
      "[15/15] [0/117] Loss D: 0.0000 / Loss G: 11.1104\n",
      "[15/15] [50/117] Loss D: 0.0000 / Loss G: 10.9044\n",
      "[15/15] [100/117] Loss D: 0.0000 / Loss G: 10.9501\n",
      "[1/15] [0/117] Loss D: 0.6804 / Loss G: 34.3162\n",
      "[1/15] [50/117] Loss D: 0.0068 / Loss G: 18.4899\n",
      "[1/15] [100/117] Loss D: 0.0028 / Loss G: 15.6603\n",
      "[2/15] [0/117] Loss D: 0.0021 / Loss G: 15.2518\n",
      "[2/15] [50/117] Loss D: 0.0011 / Loss G: 14.1163\n",
      "[2/15] [100/117] Loss D: 0.0007 / Loss G: 13.2465\n",
      "[3/15] [0/117] Loss D: 0.0008 / Loss G: 12.9872\n",
      "[3/15] [50/117] Loss D: 0.0004 / Loss G: 12.9091\n",
      "[3/15] [100/117] Loss D: 0.0003 / Loss G: 12.7379\n",
      "[4/15] [0/117] Loss D: 0.0004 / Loss G: 12.4055\n",
      "[4/15] [50/117] Loss D: 0.0002 / Loss G: 12.4232\n",
      "[4/15] [100/117] Loss D: 0.0002 / Loss G: 11.8453\n",
      "[5/15] [0/117] Loss D: 0.0002 / Loss G: 11.7735\n"
     ]
    }
   ],
   "source": [
    "for ratio in contam_ratios:\n",
    "\n",
    "  df_aucpr_local = pd.DataFrame(data = None, index = target_nums, columns = seeds)\n",
    "\n",
    "  for num in target_nums:\n",
    "    for seed in seeds:\n",
    "      #data\n",
    "      train_loader,test_X,test_y_bin = data_prepare(dataset = dataset, batch_size = batch_size, target_num = num, contam_ratio = ratio, seed = seed, hybrid = False)\n",
    "\n",
    "      #model initializaiton\n",
    "      set_seed(seed)\n",
    "      net_generator = generator(nc = nc, ngf= ngf, nz = nz)\n",
    "      net_discriminator = discriminator(nc = nc, ndf= ndf, nz = nz)\n",
    "\n",
    "      if is_cuda:\n",
    "        net_generator.cuda()\n",
    "        net_discriminator.cuda()\n",
    "\n",
    "      #weights initialization\n",
    "      net_generator.apply(weights_init)\n",
    "      net_discriminator.apply(weights_init)\n",
    "\n",
    "      optimizer_G = torch.optim.Adam(net_generator.parameters(), lr, betas=(beta1, 0.999))\n",
    "      optimizer_D = torch.optim.Adam(net_discriminator.parameters(), lr, betas=(beta1, 0.999))\n",
    "\n",
    "      fit(dataloader = train_loader, \n",
    "        net_generator = net_generator, net_discriminator = net_discriminator,\n",
    "        epochs = epochs, batch_size = batch_size, print_loss = True)\n",
    "      \n",
    "      #testing\n",
    "      score = evaluation(test_X,net_generator)\n",
    "\n",
    "      df_aucpr_local.loc[num,seed] = average_precision_score(y_true = test_y_bin, y_score = score, pos_label = -1)\n",
    "\n",
    "  #store the result\n",
    "  df_aucpr_mean.loc[:,ratio] = df_aucpr_local.apply(np.mean,axis=1)\n",
    "  df_aucpr_std.loc[:,ratio] = df_aucpr_local.apply(np.std,axis=1)\n",
    "\n",
    "df_aucpr_mean.to_csv('gan_aeplus_mean_' + dataset + '.csv',index = True)\n",
    "df_aucpr_std.to_csv('gan_aeplus_std_' + dataset + '.csv',index = True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cv_gan_aeplus_loop",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
