{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:00:39.290429Z",
     "start_time": "2020-09-10T12:00:38.159541Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "J3BHjbqDAeK_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#AUCROC,AUCPR,precision,recall,f1-score\n",
    "from sklearn.metrics import roc_curve,auc,average_precision_score,precision_score,recall_score,f1_score\n",
    "\n",
    "#gridsearch/randomsearch\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "#visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:00:40.159815Z",
     "start_time": "2020-09-10T12:00:39.292432Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-ARYrELmA6cj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Subset,DataLoader,TensorDataset\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:00:40.175544Z",
     "start_time": "2020-09-10T12:00:40.164780Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "J_P_1SmrH8Gv",
    "outputId": "88cde7d4-747f-4d6c-f981-87a9de0ca4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is off\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  is_cuda=True\n",
    "  print('GPU is on')\n",
    "else:\n",
    "  is_cuda=False\n",
    "  print('GPU is off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:00:40.190744Z",
     "start_time": "2020-09-10T12:00:40.179593Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lp22xWtgqQAn"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:00:40.221544Z",
     "start_time": "2020-09-10T12:00:40.194373Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xywA-mEoH8G0"
   },
   "outputs": [],
   "source": [
    "def data_prepare(batch_size,contam_ratio,hybrid,seed):\n",
    "    set_seed(seed)\n",
    "    data = pd.read_csv(\"E:/Research_Anomaly Detection/Taiwanese Bankruptcy Prediction (TBP)/data/TBP.csv\")\n",
    "    #change 0,1 label to 1,-1\n",
    "    data.loc[data['Flag'] == 1,'Flag'] = -1\n",
    "    data.loc[data['Flag'] == 0,'Flag'] = 1\n",
    "\n",
    "    X = np.array(data.drop(['Flag'], axis = 1))\n",
    "    #X = np.array(data.iloc[:,1:20])\n",
    "    y = np.array(data['Flag'].values)\n",
    "    \n",
    "    #reverse\n",
    "    scaler = MinMaxScaler().fit(X)\n",
    "    X = scaler.inverse_transform(X)\n",
    "\n",
    "    #split the data to training, validation and testing data (50%,20%,30%)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,shuffle = False)\n",
    "    X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size = 2/7,shuffle = False)\n",
    "\n",
    "#     X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,shuffle = True,stratify = y)\n",
    "#     X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size = 1/7,shuffle = True,stratify = y_train)\n",
    "\n",
    "    #the known positive samples before contaminating\n",
    "    known_pos_entire = sum(y_train == -1)\n",
    "    #Minmax\n",
    "    scaler=MinMaxScaler().fit(X_train)\n",
    "\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    index_contam = np.arange(len(y_train))[y_train == -1]\n",
    "    index_contam = np.random.choice(index_contam,int(contam_ratio*len(index_contam)),replace = False)\n",
    "\n",
    "    y_train[index_contam] = 1\n",
    "    #the known positive samples after contaminating\n",
    "    known_pos_sub = sum(y_train == -1)\n",
    "\n",
    "    print(f'The left true(known) positive samples in the training set:{known_pos_sub}/{known_pos_entire}\\n')\n",
    "    #normal training tensor and dataloader\n",
    "    #none-hybrid model only use \"normal\" samples in the training phase, which could be contaminated\n",
    "    if not hybrid:\n",
    "        index_subset = np.arange(len(y_train))[y_train == 1]\n",
    "    else:\n",
    "        index_subset = np.arange(len(y_train))\n",
    "    \n",
    "\n",
    "    #transform numpy to pytorch tensor\n",
    "    train_tensor = TensorDataset(torch.from_numpy(X_train).float(),torch.tensor(y_train))\n",
    "    train_tensor = Subset(train_tensor,index_subset)\n",
    "    #fitting by batches (using dataloader), there exists randomness when shuffle=True***\n",
    "    train_loader=DataLoader(train_tensor,batch_size = batch_size,shuffle = False,drop_last = True)\n",
    "    \n",
    "    #validation set\n",
    "    X_val_tensor = torch.from_numpy(X_val).float()\n",
    "    #testing set\n",
    "    X_test_tensor=torch.from_numpy(X_test).float()\n",
    "    \n",
    "    return train_loader,X_val_tensor,y_val,X_test_tensor,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:00:40.237229Z",
     "start_time": "2020-09-10T12:00:40.225257Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4h3QB1sNH8G2"
   },
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "  def __init__(self,input_size,act_fun):\n",
    "    super(generator,self).__init__()\n",
    "\n",
    "    self.encoder_1=nn.Sequential(\n",
    "      nn.Linear(input_size,input_size//4),\n",
    "      act_fun,\n",
    "      )\n",
    "    \n",
    "    self.decoder_1=nn.Sequential(\n",
    "      nn.Linear(input_size//4,input_size),\n",
    "      )\n",
    "\n",
    "    self.encoder_2=nn.Sequential(\n",
    "      nn.Linear(input_size,input_size//4),\n",
    "      act_fun,\n",
    "      )\n",
    "\n",
    "  def forward(self,input):\n",
    "    z=self.encoder_1(input)\n",
    "    X_hat=self.decoder_1(z)\n",
    "    z_hat=self.encoder_2(X_hat)\n",
    "\n",
    "    return z,X_hat,z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:00:40.253349Z",
     "start_time": "2020-09-10T12:00:40.240272Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oJVorzdrH8G4"
   },
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "  def __init__(self,input_size,act_fun):\n",
    "    super(discriminator,self).__init__()\n",
    "\n",
    "    self.encoder=nn.Sequential(\n",
    "        nn.Linear(input_size,input_size//4),\n",
    "        act_fun,\n",
    "        )\n",
    "\n",
    "    self.classifier=nn.Sequential(\n",
    "        nn.Linear(input_size//4,1),\n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "  def forward(self,input):\n",
    "    latent_vector=self.encoder(input)\n",
    "    output=self.classifier(latent_vector)\n",
    "\n",
    "    return latent_vector,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:00:40.284322Z",
     "start_time": "2020-09-10T12:00:40.258167Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "77CBVelfH8G8"
   },
   "outputs": [],
   "source": [
    "def fit(dataloader,net_generator,net_discriminator,eta,epochs,batch_size,print_loss):\n",
    "  L1_criterion = nn.L1Loss(reduction='none')\n",
    "  L2_criterion = nn.MSELoss(reduction='none')\n",
    "  BCE_criterion = nn.BCELoss(reduction='mean')\n",
    "\n",
    "  if is_cuda:\n",
    "    L1_criterion.cuda()\n",
    "    L2_criterion.cuda()\n",
    "    BCE_criterion.cuda()\n",
    "\n",
    "  loss_D_list = []\n",
    "  loss_G_list = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    for i,data in enumerate(dataloader):\n",
    "      #y_aclabel means the acquired label information (which may be contaminated)\n",
    "      X,y_aclabel = data\n",
    "      y_real = torch.FloatTensor(batch_size).fill_(0)#real label=0,size=batch_size\n",
    "      y_fake = torch.FloatTensor(batch_size).fill_(1)#fake label=1,size=batch_size\n",
    "\n",
    "      if is_cuda:\n",
    "        X = X.cuda()\n",
    "        y_aclabel = y_aclabel.cuda()\n",
    "        y_real = y_real.cuda()\n",
    "        y_fake = y_fake.cuda()\n",
    "\n",
    "      X = Variable(X)\n",
    "      y_aclabel = Variable(y_aclabel)\n",
    "      y_real = Variable(y_real)\n",
    "      y_fake = Variable(y_fake)\n",
    "\n",
    "      #zero grad for discriminator\n",
    "      net_discriminator.zero_grad()\n",
    "      #training the discriminator with real sample\n",
    "      _,output = net_discriminator(X)\n",
    "      loss_D_real = BCE_criterion(output.view(-1),y_real)\n",
    "\n",
    "      #training the discriminator with fake sample\n",
    "      _,X_hat,_ = net_generator(X)\n",
    "      _,output = net_discriminator(X_hat)\n",
    "\n",
    "      loss_D_fake = BCE_criterion(output.view(-1),y_fake)\n",
    "\n",
    "      #entire loss in discriminator\n",
    "      loss_D = (loss_D_real+loss_D_fake)/2\n",
    "      \n",
    "      loss_D.backward()\n",
    "      optimizer_D.step()\n",
    "\n",
    "      #training the generator based on the result from the discriminator\n",
    "      net_generator.zero_grad()\n",
    "\n",
    "      z,X_hat,z_hat = net_generator(X)\n",
    "\n",
    "      #latent loss\n",
    "      feature_real,_ = net_discriminator(X)\n",
    "      feature_fake,_ = net_discriminator(X_hat)\n",
    "\n",
    "      loss_G_latent = torch.mean(L2_criterion(feature_fake,feature_real),1)\n",
    "      \n",
    "      #contexutal loss\n",
    "      loss_G_contextual = torch.mean(L1_criterion(X,X_hat),1)\n",
    "      #entire loss in generator\n",
    "\n",
    "      #encoder loss\n",
    "      loss_G_encoder = torch.mean(L1_criterion(z,z_hat),1)\n",
    "      \n",
    "      loss_G = (loss_G_latent + loss_G_contextual + loss_G_encoder)/3\n",
    "      \n",
    "      loss_G_normal = (loss_G[y_aclabel == 1])\n",
    "      loss_G_anomaly = pow(loss_G[y_aclabel == -1],-1)\n",
    "      \n",
    "      if loss_G_anomaly.size(0)>0:\n",
    "        loss_G = (1-eta)*(sum(loss_G_normal)/loss_G_normal.size(0))+\\\n",
    "                eta*(sum(loss_G_anomaly)/loss_G_anomaly.size(0))\n",
    "      else:\n",
    "        loss_G = sum(loss_G_normal)/loss_G_normal.size(0)\n",
    "      \n",
    "      loss_G.backward()\n",
    "      optimizer_G.step()\n",
    "\n",
    "      if (i%50 == 0) & print_loss:\n",
    "        print('[%d/%d] [%d/%d] Loss D: %.4f / Loss G: %.4f' % (epoch+1,epochs,i,len(dataloader),loss_D,loss_G))\n",
    "\n",
    "      loss_D_list.append(loss_D)\n",
    "      loss_G_list.append(loss_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:00:40.300088Z",
     "start_time": "2020-09-10T12:00:40.288315Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "iuNcHJ0jH8G-"
   },
   "outputs": [],
   "source": [
    "def evaluation(data_tensor,model):\n",
    "  if is_cuda:\n",
    "    data_tensor = data_tensor.cuda()\n",
    "    \n",
    "  z,_,z_hat = model(data_tensor)\n",
    "\n",
    "  L1_criterion = nn.L1Loss(reduction='none')\n",
    "  score = L1_criterion(z,z_hat)\n",
    "  score = torch.sum(score,dim=1).cpu().detach().numpy()\n",
    "    \n",
    "  return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPkCKyDfH8HA"
   },
   "source": [
    "Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:00:40.315215Z",
     "start_time": "2020-09-10T12:00:40.303046Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SWmxDA8UUY3G"
   },
   "outputs": [],
   "source": [
    "seed_pool = [1,2,3,4,5]\n",
    "anomaly_ratio_pool = [0.03,0.04,0.05]\n",
    "#contam_ratio_pool = [0.98,0.5,0.0]\n",
    "contam_ratio_pool = [0.5]\n",
    "#hybrid hyper-parameter\n",
    "eta = 0.5\n",
    "#random search size\n",
    "search_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:00:40.331055Z",
     "start_time": "2020-09-10T12:00:40.318762Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5nlvzFGCH8HA"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "hyper_act_fun = [nn.Tanh(),nn.LeakyReLU()]\n",
    "hyper_lr = [1e-2,1e-3,1e-4]\n",
    "hyper_mom = [0.5,0.7,0.9]\n",
    "\n",
    "hyper_list_entire = list(product(hyper_act_fun,hyper_lr,hyper_mom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:00:40.347027Z",
     "start_time": "2020-09-10T12:00:40.333959Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "9_DOiFCZwRhH"
   },
   "outputs": [],
   "source": [
    "def random_search(hyper_list_entire,search_size,seed):\n",
    "  if search_size < len(hyper_list_entire):\n",
    "    set_seed(seed)\n",
    "    index = np.random.choice(np.arange(len(hyper_list_entire)),search_size,replace = False)\n",
    "\n",
    "    hyper_list = []\n",
    "    for i in index:\n",
    "      hyper_list.append(hyper_list_entire[i])\n",
    "  else:\n",
    "    hyper_list = hyper_list_entire\n",
    "\n",
    "  return hyper_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T12:10:20.455522Z",
     "start_time": "2020-09-10T12:00:40.350675Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "1Q-awzakH8HF",
    "outputId": "c3a2cdf9-08e6-44c0-f59f-9504e8b92857"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.008167\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.08348\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.06077\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.9)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1328\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.9)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.105\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.008222\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.0001, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.007822\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.11\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1125\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1319\n",
      "******************************\n",
      "\n",
      "\n",
      "The best hyper-parameters are: (Tanh(), 0.01, 0.9)\n",
      "\n",
      "\n",
      "Testing Phrase......\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████████████▌                                                                  | 1/5 [01:53<07:33, 113.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Precision: 35.48\n",
      "Recall: 68.75\n",
      "F1-score: 46.81\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 29.27\n",
      "Recall: 75.0\n",
      "F1-score: 42.11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 23.3\n",
      "Recall: 75.0\n",
      "F1-score: 35.56\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1277\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.09742\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.0001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.009683\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1149\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.136\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.9)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1154\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.05299\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1275\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.04797\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.009478\n",
      "******************************\n",
      "\n",
      "\n",
      "The best hyper-parameters are: (LeakyReLU(negative_slope=0.01), 0.01, 0.7)\n",
      "\n",
      "\n",
      "Testing Phrase......\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|█████████████████████████████████▏                                                 | 2/5 [03:49<05:42, 114.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Precision: 33.87\n",
      "Recall: 65.62\n",
      "F1-score: 44.68\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 32.93\n",
      "Recall: 84.38\n",
      "F1-score: 47.37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 26.21\n",
      "Recall: 84.38\n",
      "F1-score: 40.0\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.9)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1247\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.9)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.116\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1166\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.9)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1045\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.0703\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.05284\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.007425\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.007466\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.01583\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1219\n",
      "******************************\n",
      "\n",
      "\n",
      "The best hyper-parameters are: (LeakyReLU(negative_slope=0.01), 0.01, 0.9)\n",
      "\n",
      "\n",
      "Testing Phrase......\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 3/5 [05:48<03:50, 115.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Precision: 37.1\n",
      "Recall: 71.88\n",
      "F1-score: 48.94\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 31.71\n",
      "Recall: 81.25\n",
      "F1-score: 45.61\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 27.18\n",
      "Recall: 87.5\n",
      "F1-score: 41.48\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.007807\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.03693\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.0001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.008249\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.9)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1428\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.06304\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.06795\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.0001, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.008258\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.03673\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1455\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.06382\n",
      "******************************\n",
      "\n",
      "\n",
      "The best hyper-parameters are: (LeakyReLU(negative_slope=0.01), 0.01, 0.7)\n",
      "\n",
      "\n",
      "Testing Phrase......\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 4/5 [07:43<01:55, 115.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Precision: 35.48\n",
      "Recall: 68.75\n",
      "F1-score: 46.81\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 31.71\n",
      "Recall: 81.25\n",
      "F1-score: 45.61\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 26.21\n",
      "Recall: 84.38\n",
      "F1-score: 40.0\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.9)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1053\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1124\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1348\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.01, 0.9)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1236\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.01, 0.9)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.1207\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.0001, 0.9)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.01279\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (LeakyReLU(negative_slope=0.01), 0.001, 0.5)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.04438\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.009997\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.001, 0.7)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.0694\n",
      "******************************\n",
      "\n",
      "\n",
      "Finding Optimal Hyper-parameters......Current Candidates: (Tanh(), 0.0001, 0.9)\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n",
      "The metric value corresponded to the hyper-parameters is :0.01255\n",
      "******************************\n",
      "\n",
      "\n",
      "The best hyper-parameters are: (LeakyReLU(negative_slope=0.01), 0.01, 0.7)\n",
      "\n",
      "\n",
      "Testing Phrase......\n",
      "The left true(known) positive samples in the training set:85/170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [09:40<00:00, 116.01s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [09:40<00:00, 580.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Precision: 35.48\n",
      "Recall: 68.75\n",
      "F1-score: 46.81\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 30.49\n",
      "Recall: 78.12\n",
      "F1-score: 43.86\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision: 25.24\n",
      "Recall: 81.25\n",
      "F1-score: 38.52\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for contam_ratio in tqdm(contam_ratio_pool):\n",
    "  df_result = pd.DataFrame(data = None,index = ['AUCPR'] + anomaly_ratio_pool,columns = seed_pool) \n",
    "\n",
    "  for seed in tqdm(seed_pool):\n",
    "    #############################################seleting the best hyper-parameters in validation set#############################################\n",
    "    metric_value_list = list()\n",
    "    hyper_list = random_search(hyper_list_entire,search_size,seed)\n",
    "    for i in range(len(hyper_list)):\n",
    "      try:\n",
    "        print(f'Finding Optimal Hyper-parameters......Current Candidates: {hyper_list[i]}')\n",
    "        act_fun,lr,mom = hyper_list[i]\n",
    "        #data\n",
    "        train_loader,X_val_tensor,y_val,_,_ = data_prepare(batch_size,contam_ratio,hybrid = True,seed = seed)\n",
    "        #model initialization\n",
    "        set_seed(seed)\n",
    "        net_generator=generator(input_size = X_val_tensor.size(1),act_fun = act_fun)\n",
    "        net_discriminator=discriminator(input_size = X_val_tensor.size(1),act_fun = act_fun)\n",
    "\n",
    "        if is_cuda:\n",
    "          net_generator.cuda()\n",
    "          net_discriminator.cuda()\n",
    "\n",
    "        optimizer_G = torch.optim.SGD(net_generator.parameters(),lr = lr,momentum = mom)\n",
    "        optimizer_D = torch.optim.SGD(net_discriminator.parameters(),lr = lr,momentum = mom)\n",
    "        #fitting\n",
    "        fit(dataloader = train_loader,net_generator = net_generator,net_discriminator = net_discriminator,\n",
    "          eta = eta,epochs = epochs,batch_size = batch_size,print_loss = False)\n",
    "        \n",
    "        #evaluation\n",
    "        score = evaluation(data_tensor = X_val_tensor,model = net_generator)\n",
    "        metric_value = average_precision_score(y_true = y_val,y_score = score,pos_label = -1)\n",
    "        metric_value_list.append(metric_value)\n",
    "\n",
    "        print(f'The metric value corresponded to the hyper-parameters is :{metric_value:{.4}}')\n",
    "        print('******************************')\n",
    "        print('\\n')\n",
    "      except:\n",
    "        #keep the right index\n",
    "        metric_value_list.append(0)\n",
    "        pass\n",
    "      continue\n",
    "\n",
    "    best_hyper_params=hyper_list[metric_value_list.index(max(metric_value_list))]\n",
    "    print(f'The best hyper-parameters are: {best_hyper_params}')\n",
    "    print('\\n')\n",
    "    ###################################################################testing#########################################################################\n",
    "    print('Testing Phrase......')\n",
    "    act_fun,lr,mom = best_hyper_params\n",
    "\n",
    "    #data\n",
    "    train_loader,_,_,X_test_tensor,y_test = data_prepare(batch_size,contam_ratio,hybrid = True,seed = seed)\n",
    "    \n",
    "    #model initialization, there exists randomness because of weight initialization***\n",
    "    set_seed(seed)\n",
    "    net_generator=generator(input_size = X_test_tensor.size(1),act_fun = act_fun)\n",
    "    net_discriminator=discriminator(input_size = X_test_tensor.size(1),act_fun = act_fun)\n",
    "\n",
    "    if is_cuda:\n",
    "      net_generator.cuda()\n",
    "      net_discriminator.cuda()\n",
    "\n",
    "    optimizer_G = torch.optim.SGD(net_generator.parameters(),lr = lr,momentum = mom)\n",
    "    optimizer_D = torch.optim.SGD(net_discriminator.parameters(),lr = lr,momentum = mom)\n",
    "    #fitting\n",
    "    fit(dataloader = train_loader,net_generator = net_generator,net_discriminator = net_discriminator,\n",
    "      eta = eta,epochs = epochs,batch_size = batch_size,print_loss = False)\n",
    "    #evaluation\n",
    "    score = evaluation(data_tensor = X_test_tensor,model = net_generator)\n",
    "    \n",
    "    #store the result\n",
    "    #AUCPR\n",
    "    df_result.loc['AUCPR',seed] = average_precision_score(y_true = y_test,y_score = score,pos_label = -1)\n",
    "    #F1\n",
    "    for anomaly_ratio in anomaly_ratio_pool:\n",
    "        threshold = score[np.argsort(-score)][int(anomaly_ratio*len(score))]\n",
    "        \n",
    "        y_pred = np.ones(len(score))\n",
    "        y_pred[score >= threshold] = -1\n",
    "        \n",
    "        print('\\n')\n",
    "        print(f'Precision: {round(precision_score(y_pred = y_pred, y_true = y_test, pos_label= -1)*100,2)}')\n",
    "        print(f'Recall: {round(recall_score(y_pred = y_pred, y_true = y_test, pos_label= -1)*100,2)}')\n",
    "        print(f'F1-score: {round(f1_score(y_pred = y_pred, y_true = y_test, pos_label= -1)*100,2)}')\n",
    "        print('\\n')\n",
    "\n",
    "        df_result.loc[anomaly_ratio,seed] = f1_score(y_pred = y_pred,y_true = y_test,pos_label = -1) \n",
    "  \n",
    "  #mean & sd\n",
    "  df_result['mean'] = np.mean(df_result.loc[:,seed_pool],axis = 1)\n",
    "  df_result['std'] = np.std(df_result.loc[:,seed_pool],axis = 1)\n",
    "  df_result = round(df_result.astype('float64')*100,2)\n",
    "\n",
    "  file_path = 'E:/Research_Anomaly Detection/Taiwanese Bankruptcy Prediction (TBP)/result/' + 'TBP_HGAN_' + str(contam_ratio) + '.csv'\n",
    "  df_result.to_csv(file_path,index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CCFD_HGAN_tuneloop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
